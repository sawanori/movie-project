from google import genai
from google.genai import types
from PIL import Image
import io
import json
import httpx
import logging
from pathlib import Path

from app.core.config import settings

logger = logging.getLogger(__name__)

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆmovie-maker-api ã®è¦ª = movie-projectï¼‰
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent  # movie-project/


def load_prompt_template(
    provider: str,
    mode: str = "story",
    subject_type: str | None = None,
    animation_category: str | None = None,
    animation_template: str | None = None
) -> dict:
    """
    å‹•ç”»ç”Ÿæˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€

    Args:
        provider: "runway", "veo", "domoai", ã¾ãŸã¯ "piapi_kling"
        mode: "story"ï¼ˆã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒœãƒ¼ãƒ‰ç”¨ãƒ»é€£ç¶šæ€§é‡è¦–ï¼‰ã¾ãŸã¯ "scene"ï¼ˆã‚·ãƒ¼ãƒ³ç”Ÿæˆç”¨ãƒ»ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆé‡è¦–ï¼‰
        subject_type: "person"/"object"/"animation"ï¼ˆsceneãƒ¢ãƒ¼ãƒ‰æ™‚ã®ã¿ä½¿ç”¨ï¼‰
        animation_category: "2d" ã¾ãŸã¯ "3d"ï¼ˆanimationé¸æŠæ™‚ã®ã¿ä½¿ç”¨ï¼‰
        animation_template: "A-1"ã€œ"B-4"ï¼ˆanimationé¸æŠæ™‚ã®ã¿ä½¿ç”¨ï¼‰

    Returns:
        dict: {
            "reference_rule": str,
            "clip_specific_template": str,
            "negative_prompt": str | None,
            "style_keywords": str | None  # ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã¿
        }
    """
    # ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å ´åˆ
    if subject_type == "animation" and animation_category and animation_template:
        return _load_animation_template(animation_category, animation_template, provider)

    # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é¸æŠ
    if mode not in ("story", "scene"):
        mode = "story"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯story

    # sceneãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€subject_typeã«å¿œã˜ãŸã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
    if mode == "scene":
        if subject_type in ("person", "object"):
            template_dir = PROJECT_ROOT / "docs" / "prompt" / mode / subject_type
        elif subject_type == "animation":
            # animationé¸æŠã§ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæœªæŒ‡å®šã®å ´åˆã¯personã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            template_dir = PROJECT_ROOT / "docs" / "prompt" / mode / "person"
            logger.info(f"Animation without template, falling back to person template")
        else:
            template_dir = PROJECT_ROOT / "docs" / "prompt" / mode / "person"
    else:
        template_dir = PROJECT_ROOT / "docs" / "prompt" / mode

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å
    provider_template_map = {
        "veo": "veo_api_template.md",
        "domoai": "domoai_api_template.md",
        "piapi_kling": "kling_api_template.md",
    }
    template_filename = provider_template_map.get(provider, "runway_api_template.md")
    template_path = template_dir / template_filename

    if not template_path.exists():
        logger.warning(f"Template file not found: {template_path}, using default")
        return {
            "reference_rule": "",
            "clip_specific_template": "",
            "negative_prompt": None,
            "style_keywords": None,
            "quality_boosters": None
        }

    content = template_path.read_text(encoding="utf-8")

    result = {
        "reference_rule": "",
        "clip_specific_template": "",
        "negative_prompt": None,
        "style_keywords": None,
        "quality_boosters": None
    }

    # REFERENCE RULE ã¾ãŸã¯ SINGLE IMAGE RULE ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
    if "REFERENCE RULE" in content:
        start = content.find("REFERENCE RULE")
        end = content.find("CLIP SPECIFIC", start)
        if end > start:
            result["reference_rule"] = content[start:end].strip()
    elif "SINGLE IMAGE RULE" in content:
        # ã‚·ãƒ¼ãƒ³ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆ1æšã®ç”»åƒç”¨ï¼‰
        start = content.find("SINGLE IMAGE RULE")
        end = content.find("CLIP SPECIFIC", start)
        if end > start:
            result["reference_rule"] = content[start:end].strip()

    # CLIP SPECIFIC ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æŠ½å‡º
    if "CLIP SPECIFIC" in content:
        start = content.find("CLIP SPECIFIC (edit only this block):")
        if start != -1:
            # æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ---ï¼‰ã¾ã§å–å¾—
            end = content.find("---", start)
            if end == -1:
                end = content.find("## NEGATIVE PROMPT", start)
            if end == -1:
                end = content.find("## ä¾‹", start)
            if end > start:
                result["clip_specific_template"] = content[start:end].strip()

    # NEGATIVE PROMPT ã‚’æŠ½å‡ºï¼ˆRunwayã®ã¿ï¼‰
    if "NEGATIVE PROMPT" in content:
        start = content.find("## NEGATIVE PROMPT")
        if start != -1:
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…å®¹ã‚’å–å¾—
            lines_start = content.find("\n", start) + 1
            end = content.find("---", lines_start)
            if end > lines_start:
                result["negative_prompt"] = content[lines_start:end].strip()

    logger.info(f"Loaded prompt template for provider: {provider}, mode: {mode}, subject_type: {subject_type}")
    return result


def _load_animation_template(category: str, template_id: str, provider: str = "runway") -> dict:
    """
    ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€

    Args:
        category: "2d" ã¾ãŸã¯ "3d"
        template_id: "A-1"ã€œ"A-4"ï¼ˆ2Dï¼‰ã¾ãŸã¯ "B-1"ã€œ"B-4"ï¼ˆ3Dï¼‰
        provider: "runway", "veo", "domoai", ã¾ãŸã¯ "piapi_kling"

    Returns:
        dict: {
            "reference_rule": str,
            "clip_specific_template": str,
            "negative_prompt": str | None,
            "style_keywords": str
        }
    """
    import re

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°
    template_file_maps = {
        "runway": {
            "A-1": "A-1_modern_tv_anime.md",
            "A-2": "A-2_ghibli_style.md",
            "A-3": "A-3_90s_retro.md",
            "A-4": "A-4_flat_design.md",
            "B-1": "B-1_photorealistic.md",
            "B-2": "B-2_game_ue5.md",
            "B-3": "B-3_pixar_style.md",
            "B-4": "B-4_low_poly_ps1.md",
        },
        "veo": {
            "A-1": "A-1_modern_vtuber.md",
            "A-2": "A-2_ghibli.md",
            "A-3": "A-3_90s_retro.md",
            "A-4": "A-4_flat_simple.md",
            "B-1": "B-1_photorealistic.md",
            "B-2": "B-2_ue5_game.md",
            "B-3": "B-3_pixar.md",
            "B-4": "B-4_low_poly_ps1.md",
        },
        "domoai": {
            "A-1": "A-1_japanese_anime.md",
            "A-2": "A-2_flat_color_anime.md",
            "A-3": "A-3_90s_retro.md",
            "A-4": "A-4_pixel_art.md",
            "B-1": "B-1_realistic.md",
            "B-2": "B-2_cartoon_game.md",
            "B-3": "B-3_3d_anime.md",
            "B-4": "B-4_chibi_deformed.md",
        },
        "piapi_kling": {
            "A-1": "A-1_modern_tv_anime.md",
            "A-2": "A-2_ghibli_style.md",
            "A-3": "A-3_90s_retro.md",
            "A-4": "A-4_flat_design.md",
            "B-1": "B-1_photorealistic.md",
            "B-2": "B-2_game_ue5.md",
            "B-3": "B-3_pixar_style.md",
            "B-4": "B-4_low_poly_ps1.md",
        },
    }

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ•ã‚©ãƒ«ãƒ€åãŒç•°ãªã‚‹å ´åˆï¼‰
    provider_folder_map = {
        "runway": "runway",
        "veo": "veo",
        "domoai": "domo",  # DomoAIã¯domoãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½¿ç”¨
        "piapi_kling": "kling",  # PiAPI Klingã¯klingãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½¿ç”¨
    }

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ãŸãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å–å¾—ï¼ˆä¸æ˜ãªå ´åˆã¯runwayï¼‰
    file_map = template_file_maps.get(provider, template_file_maps["runway"])
    filename = file_map.get(template_id)
    folder_name = provider_folder_map.get(provider, provider)

    if not filename:
        logger.warning(f"Unknown animation template ID: {template_id}")
        return {
            "reference_rule": "",
            "clip_specific_template": "",
            "negative_prompt": None,
            "style_keywords": ""
        }

    template_path = PROJECT_ROOT / "docs" / "prompt" / "scene" / "anime" / category / folder_name / filename

    if not template_path.exists():
        logger.warning(f"Animation template file not found: {template_path}")
        return {
            "reference_rule": "",
            "clip_specific_template": "",
            "negative_prompt": None,
            "style_keywords": ""
        }

    content = template_path.read_text(encoding="utf-8")

    result = {
        "reference_rule": "",
        "clip_specific_template": "",
        "negative_prompt": None,
        "style_keywords": "",
        "quality_boosters": ""  # DomoAIç”¨ã®å“è³ªå‘ä¸Šãƒ–ãƒ¼ã‚¹ã‚¿ãƒ¼
    }

    # å“è³ªå‘ä¸Šãƒ–ãƒ¼ã‚¹ã‚¿ãƒ¼ï¼ˆMagic Wordsï¼‰ã‚’æŠ½å‡ºï¼ˆDomoAIç”¨ï¼‰
    if "## å“è³ªå‘ä¸Šãƒ–ãƒ¼ã‚¹ã‚¿ãƒ¼" in content:
        start = content.find("## å“è³ªå‘ä¸Šãƒ–ãƒ¼ã‚¹ã‚¿ãƒ¼")
        end = content.find("---", start)
        if end > start:
            booster_section = content[start:end]
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            code_blocks = re.findall(r'```\n?(.*?)\n?```', booster_section, re.DOTALL)
            if code_blocks:
                # æœ€åˆã®2ã¤ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’çµåˆï¼ˆä¸€èˆ¬ + ã‚¹ã‚¿ã‚¤ãƒ«ç‰¹åŒ–ï¼‰
                boosters = []
                for block in code_blocks[:2]:
                    boosters.append(block.strip())
                result["quality_boosters"] = ", ".join(boosters)

    # ã‚¹ã‚¿ã‚¤ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    if "## ã‚¹ã‚¿ã‚¤ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in content:
        start = content.find("## ã‚¹ã‚¿ã‚¤ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
        end = content.find("---", start)
        if end > start:
            keywords_section = content[start:end]
            # ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆå†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            keywords = re.findall(r'`([^`]+)`', keywords_section)
            result["style_keywords"] = ", ".join(keywords)

    # TEXT PROMPT ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ SINGLE IMAGE RULE ã¨ CLIP SPECIFIC ã‚’æŠ½å‡º
    if "## TEXT PROMPT" in content:
        start = content.find("## TEXT PROMPT")
        end = content.find("---", start)
        if end > start:
            template_section = content[start:end]

            # SINGLE IMAGE RULE ã‚’æŠ½å‡º
            if "SINGLE IMAGE RULE" in template_section:
                rule_start = template_section.find("SINGLE IMAGE RULE")
                rule_end = template_section.find("CLIP SPECIFIC", rule_start)
                if rule_end > rule_start:
                    result["reference_rule"] = template_section[rule_start:rule_end].strip()

            # CLIP SPECIFIC ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æŠ½å‡º
            if "CLIP SPECIFIC" in template_section:
                clip_start = template_section.find("CLIP SPECIFIC (edit only this block):")
                if clip_start != -1:
                    # Final note: ã®è¡Œã¾ã§å–å¾—ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ‚äº†ï¼‰
                    clip_section = template_section[clip_start:]
                    # Final note ã§çµ‚ã‚ã‚‹è¡Œã‚’è¦‹ã¤ã‘ã‚‹
                    final_note_match = re.search(r'Final note:.*', clip_section)
                    if final_note_match:
                        clip_end = final_note_match.end()
                        result["clip_specific_template"] = clip_section[:clip_end].strip()
                    else:
                        result["clip_specific_template"] = clip_section.strip()

    # æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    elif "## ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ" in content:
        start = content.find("## ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
        end = content.find("---", start)
        if end > start:
            template_section = content[start:end]
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æŠ½å‡º
            code_start = template_section.find("```")
            code_end = template_section.rfind("```")
            if code_start != -1 and code_end > code_start:
                template_text = template_section[code_start+3:code_end].strip()
                if template_text.startswith("\n"):
                    template_text = template_text[1:]
                result["clip_specific_template"] = template_text

    # ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    if "## ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in content:
        start = content.find("## ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
        end = content.find("---", start)
        if end == -1:
            end = content.find("## æ³¨æ„äº‹é …", start)
        if end == -1:
            end = content.find("## å®Ÿä¾‹", start)
        if end > start:
            neg_section = content[start:end]
            # âŒã§å§‹ã¾ã‚‹è¡Œã‚’æŠ½å‡º
            negatives = re.findall(r'âŒ\s*(.+)', neg_section)
            if negatives:
                result["negative_prompt"] = "Avoid: " + ", ".join(negatives)

    # reference_rule ãŒç©ºã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not result["reference_rule"]:
        result["reference_rule"] = f"""SINGLE IMAGE RULE (do not remove):
Use the source image as the foundation for the video.
Preserve the character design, art style, and color palette from the input image.
Focus on motion and camera work only - do NOT describe character appearance.
Animation style: {template_id} ({category.upper()})
Style keywords: {result["style_keywords"]}"""

    logger.info(f"Loaded animation template: {template_id} (category: {category}, provider: {provider}, folder: {folder_name}, quality_boosters: {bool(result['quality_boosters'])})")
    return result


def get_gemini_client() -> genai.Client:
    """Initialize Gemini client."""
    return genai.Client(api_key=settings.GOOGLE_API_KEY)


async def optimize_prompt(prompt: str, template_id: str | None = None) -> str:
    """
    Optimize prompt for video generation using Gemini 3 Flash.
    
    Args:
        prompt (str): User input prompt.
        template_id (str | None): Template ID to add context (optional).
        
    Returns:
        str: Optimized prompt.
    """
    client = get_gemini_client()
    
    system_instruction = (
        "You are an expert prompt engineer for video generation AI (like KlingAI, Sora). "
        "Your task is to expand the user's input into a detailed, descriptive prompt suitable for generating a high-quality 5-second video. "
        "Focus on visual details, lighting, camera movement, and atmosphere. "
        "Keep the output in English, even if the input is in Japanese. "
        "Do not include any explanations, just return the optimized prompt string."
    )

    try:
        response = client.models.generate_content(
            model="gemini-3-flash-preview",
            contents=prompt,
            config=types.GenerateContentConfig(
                system_instruction=system_instruction,
                temperature=0.7,
            )
        )
        
        return response.text
    except Exception as e:
        print(f"Gemini optimization failed: {e}")
        return prompt  # Fallback to original prompt


async def generate_image(
    prompt: str,
    reference_image_urls: list[str] | None = None
) -> Image.Image | None:
    """
    Generate an image using Gemini 3 Pro (Nano Banana Pro).

    Args:
        prompt (str): Image generation prompt.
        reference_image_urls (list[str] | None): Optional reference image URLs (max 3).
            When provided, Gemini will use these images as style/content references,
            blending their visual characteristics in the generated image.

    Returns:
        Image.Image | None: Generated PIL Image or None if failed.
    """
    client = get_gemini_client()

    try:
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰
        contents = []

        # å‚ç…§ç”»åƒãŒã‚ã‚‹å ´åˆã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã¨ã—ã¦æ¸¡ã™ï¼ˆæœ€å¤§3æšï¼‰
        if reference_image_urls and len(reference_image_urls) > 0:
            loaded_images = []
            for i, img_url in enumerate(reference_image_urls[:3]):  # æœ€å¤§3æš
                logger.info(f"Downloading reference image {i+1} for image generation: {img_url}")
                try:
                    async with httpx.AsyncClient() as http_client:
                        img_response = await http_client.get(img_url, timeout=30.0)
                        img_response.raise_for_status()
                        image_data = img_response.content

                    # å‚ç…§ç”»åƒã‚’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«è¿½åŠ 
                    contents.append(types.Part.from_bytes(data=image_data, mime_type="image/jpeg"))
                    loaded_images.append(i + 1)
                except Exception as e:
                    logger.warning(f"Failed to download reference image {i+1}: {e}")

            if loaded_images:
                # å‚ç…§ç”»åƒã®æšæ•°ã«å¿œã˜ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª¿æ•´
                if len(loaded_images) == 1:
                    edit_prompt = f"""Based on the provided reference image, generate a new image with the following modifications while preserving the core visual style, colors, and aesthetic of the original:

{prompt}

IMPORTANT:
- Maintain the same color palette and lighting style as the reference image
- Keep the same photographic quality and artistic style
- Preserve the overall mood and atmosphere
- Apply the requested changes while keeping visual consistency with the reference"""
                else:
                    # è¤‡æ•°ç”»åƒã®å ´åˆã¯æ›ã‘åˆã‚ã›ã‚’æŒ‡ç¤º
                    edit_prompt = f"""You are provided with {len(loaded_images)} reference images. Generate a new image that intelligently COMBINES and BLENDS elements from ALL provided reference images according to the following instructions:

{prompt}

MULTI-REFERENCE BLENDING RULES:
- Image 1: Use as the PRIMARY style/aesthetic reference (color palette, lighting, mood)
- Image 2: Extract key visual elements, subjects, or compositional ideas to incorporate
{"- Image 3: Additional style influence or detail reference" if len(loaded_images) >= 3 else ""}

IMPORTANT:
- Create a COHESIVE blend that feels natural, not collaged
- Maintain consistent lighting and color grading across all blended elements
- Preserve the photographic quality from Image 1
- Intelligently merge subjects/elements from other images
- The final result should look like a single, professionally composed image"""

                contents.append(edit_prompt)
                logger.info(f"{len(loaded_images)} reference image(s) added to image generation request")
            else:
                contents = [prompt]
        else:
            contents = [prompt]

        # Use Gemini 3 Pro Image (Nano Banana Pro) model
        response = client.models.generate_content(
            model="gemini-3-pro-image-preview",
            contents=contents,
            config=types.GenerateContentConfig(
                response_modalities=["image", "text"],
            )
        )

        for part in response.parts:
            if part.inline_data:
                return part.as_image()

        return None

    except Exception as e:
        logger.error(f"Gemini image generation failed: {e}")
        return None


# ===== AIä¸»å°ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°ç”¨é–¢æ•° =====

async def suggest_stories_from_image(image_url: str) -> list[str]:
    """
    ç”»åƒã‹ã‚‰ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å€™è£œã‚’5ã¤ç”Ÿæˆ

    Args:
        image_url: åˆ†æå¯¾è±¡ã®ç”»åƒURL

    Returns:
        list[str]: æ—¥æœ¬èªã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å€™è£œãƒªã‚¹ãƒˆ
    """
    client = get_gemini_client()

    system_prompt = """
ã“ã®ç”»åƒã‚’åˆ†æã—ã¦ã€5ç§’é–“ã®çŸ­ã„å‹•ç”»ã«ã§ããã†ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’5ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚

ãƒ«ãƒ¼ãƒ«:
- ç”»åƒã«å†™ã£ã¦ã„ã‚‹äººç‰©/å‹•ç‰©/ç‰©ã®å‹•ãã‚’æƒ³åƒã™ã‚‹
- ã‚·ãƒ³ãƒ—ãƒ«ã§å®Ÿç¾å¯èƒ½ãªå‹•ãã«ã™ã‚‹ï¼ˆå¤§ããªå ´é¢è»¢æ›ã¯é¿ã‘ã‚‹ï¼‰
- æ—¥æœ¬èªã§ã€1æ–‡ã§ç°¡æ½”ã«æ›¸ãï¼ˆ15ã€œ25æ–‡å­—ç¨‹åº¦ï¼‰
- å‹•ãã‚„å¤‰åŒ–ã‚’å…·ä½“çš„ã«æå†™ã™ã‚‹

JSONé…åˆ—å½¢å¼ã§å‡ºåŠ›ï¼ˆèª¬æ˜ã‚„å‰ç½®ãã¯ä¸è¦ï¼‰:
["ã‚¹ãƒˆãƒ¼ãƒªãƒ¼1", "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼2", "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼3", "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼4", "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼5"]
"""

    try:
        # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        async with httpx.AsyncClient() as http_client:
            img_response = await http_client.get(image_url, timeout=30.0)
            img_response.raise_for_status()
            image_data = img_response.content

        # Geminiã«é€ä¿¡
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=[
                types.Part.from_bytes(data=image_data, mime_type="image/jpeg"),
                system_prompt
            ],
            config=types.GenerateContentConfig(temperature=0.8)
        )

        # JSONã‚’ãƒ‘ãƒ¼ã‚¹
        result_text = response.text.strip()
        # ```json ... ``` ã‚’é™¤å»
        if result_text.startswith("```"):
            result_text = result_text.split("```")[1]
            if result_text.startswith("json"):
                result_text = result_text[4:]

        return json.loads(result_text)

    except Exception as e:
        logger.exception(f"Story suggestion failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return [
            "ã‚†ã£ãã‚Šã¨ã‚«ãƒ¡ãƒ©ç›®ç·šã«ãªã‚‹",
            "é¢¨ã§é«ªãŒãªã³ã",
            "å¾®ç¬‘ã¿ã‹ã‚‰é©šã„ãŸè¡¨æƒ…ã«å¤‰ã‚ã‚‹",
            "æ‰‹ã‚’æŒ¯ã‚‹å‹•ä½œã‚’ã™ã‚‹",
            "æ·±å‘¼å¸ã—ã¦ãƒªãƒ©ãƒƒã‚¯ã‚¹ã™ã‚‹"
        ]


async def analyze_image_for_base_prompt(image_url: str) -> str:
    """
    ç”»åƒã‚’è§£æã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆï¼ˆIdentity Anchorä»˜ãï¼‰

    Args:
        image_url: åˆ†æå¯¾è±¡ã®ç”»åƒURL

    Returns:
        str: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã€èƒŒæ™¯ã€ç”»é¢¨ã‚’å«ã‚€è©³ç´°ãªèª¬æ˜ï¼ˆè‹±èªï¼‰
              Identity Anchorï¼ˆçµ¶å¯¾å¤‰æ›´ç¦æ­¢é …ç›®ï¼‰ã‚’å«ã‚€
    """
    client = get_gemini_client()

    system_prompt = """
Analyze this image with EXTREME PRECISION for use as an "Identity Anchor" in AI video generation.

Your description will be used to ensure the EXACT same subject appears in all generated frames.
ANY deviation in the generated content will be considered a CRITICAL FAILURE.

Describe with MAXIMUM DETAIL:

ã€IDENTITY ANCHOR - ABSOLUTELY UNCHANGEABLEã€‘
1. FACE (if human/animal):
   - Exact facial structure (face shape, jawline, cheekbones)
   - Eye details (shape, color, size, spacing, eyelid type)
   - Nose details (shape, size, bridge)
   - Mouth/lips details (shape, size, color)
   - Eyebrows (shape, thickness, color)
   - Skin tone (exact shade)
   - Any distinctive features (moles, freckles, scars)
   - Apparent age and gender

2. HAIR:
   - Exact color (include highlights, roots if visible)
   - Length and style
   - Texture (straight, wavy, curly)
   - Parting and arrangement

3. BODY:
   - Body type and proportions
   - Visible skin areas

4. CLOTHING & ACCESSORIES:
   - Every piece of clothing with colors and patterns
   - All accessories (jewelry, glasses, bags, etc.)
   - Textures and materials

5. BACKGROUND ELEMENTS:
   - Location type
   - All visible objects
   - Colors and atmosphere

6. TECHNICAL:
   - Photography style (portrait, candid, etc.)
   - Lighting direction and quality
   - Color grading/tone

Format your response as:
---IDENTITY_ANCHOR_START---
[Your detailed description here as one comprehensive paragraph]
---IDENTITY_ANCHOR_END---

PROTECTION_LEVEL: MAXIMUM
WARNING: Any AI-generated frames MUST preserve EVERY detail listed above.
The subject's identity, appearance, and all visual elements are LOCKED and IMMUTABLE.
"""

    try:
        async with httpx.AsyncClient() as http_client:
            img_response = await http_client.get(image_url, timeout=30.0)
            img_response.raise_for_status()
            image_data = img_response.content

        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=[
                types.Part.from_bytes(data=image_data, mime_type="image/jpeg"),
                system_prompt
            ],
            config=types.GenerateContentConfig(temperature=0.2)  # ä½ã‚ã§ç²¾åº¦é‡è¦–
        )

        return response.text.strip()

    except Exception as e:
        logger.exception(f"Image analysis failed: {e}")
        raise


async def generate_storyboard_prompts(base_prompt: str, story_text: str) -> list[dict]:
    """
    ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‹ã‚‰3ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†ã®æ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ

    Args:
        base_prompt: ç”»åƒè§£æã‹ã‚‰å¾—ãŸãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆIdentity Anchorå«ã‚€ï¼‰
        story_text: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼

    Returns:
        list[dict]: 3ã¤ã®æ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå„ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
    """
    client = get_gemini_client()

    system_prompt = f"""
You are an expert prompt engineer for AI image generation with STRICT identity preservation requirements.

Based on the following Identity Anchor and story, create 3 sequential frame prompts.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€IDENTITY ANCHOR - PROTECTED ELEMENTS (DO NOT MODIFY)ã€‘
{base_prompt}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ã€Story (the ONLY thing that can change)ã€‘
{story_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ CRITICAL PROTECTION RULES - VIOLATION = FAILURE âš ï¸
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”’ ABSOLUTELY LOCKED (Cannot change across ANY frame):
- Face structure, features, and all facial details
- Eye color, shape, and characteristics
- Hair color, length, style, and texture
- Skin tone and any distinctive marks
- Body type and proportions
- ALL clothing items and their colors/patterns
- ALL accessories (jewelry, glasses, bags, etc.)
- Background location and objects
- Art style, lighting quality, and color grading

âœ… ALLOWED TO CHANGE (Only these):
- Facial expression (smile, surprise, etc.)
- Eye direction (looking left, right, up, down)
- Head angle (slight tilt or turn)
- Body pose (arm position, stance)
- Natural movements (blinking, breathing)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CRITICAL RULES FOR HUMAN SUBJECTS:
- ALWAYS include "natural blinking" or "subtle eye blink" in [Action]
- This ensures realistic human movement in video

For each frame, create a structured prompt:

[Scene] COPY EXACTLY from Identity Anchor - same background
[Element] COPY EXACTLY from Identity Anchor - same subject with ALL details
[Action] ONLY pose/expression change - NOTHING else
[Style] cinematic, photorealistic, SAME lighting as original

Frame progression:
- Frame 1: EXACT match to original image
- Frame 2: Slight movement (same identity, small action change)
- Frame 3: Final pose (same identity, story conclusion)

VERIFICATION CHECKLIST (apply to each frame):
â–¡ Face identical to original? âœ“
â–¡ Hair identical to original? âœ“
â–¡ Clothing identical to original? âœ“
â–¡ Accessories identical to original? âœ“
â–¡ Background identical to original? âœ“
â–¡ Only pose/expression changed? âœ“

Output as JSON array with 3 objects:
[
  {{
    "frame": 1,
    "scene": "...",
    "element": "...",
    "action": "...",
    "style": "...",
    "full_prompt": "[Scene] ... [Element] ... [Action] ... [Style] ... IMPORTANT: Preserve exact identity from reference."
  }},
  ...
]

Output JSON only.
"""

    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=system_prompt,
            config=types.GenerateContentConfig(temperature=0.7)
        )

        result_text = response.text.strip()
        if result_text.startswith("```"):
            result_text = result_text.split("```")[1]
            if result_text.startswith("json"):
                result_text = result_text[4:]

        prompts = json.loads(result_text)

        if len(prompts) != 3:
            raise ValueError(f"Expected 3 prompts, got {len(prompts)}")

        return prompts

    except Exception as e:
        logger.exception(f"Storyboard generation failed: {e}")
        raise


async def generate_4scene_storyboard(
    image_url: str,
    mood: str | None = None,
    video_provider: str = "runway"
) -> dict:
    """
    ã€æ˜ ç”»ç›£ç£ãƒ¢ãƒ¼ãƒ‰ã€‘1æšã®ç”»åƒã‹ã‚‰èµ·æ‰¿è»¢çµ4ã‚·ãƒ¼ãƒ³ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒœãƒ¼ãƒ‰ã‚’ç”Ÿæˆ

    ä¸–ç•Œæœ€é«˜å³°ã®æ˜ ç”»ç›£ç£å…¼å‹•ç”»ç”ŸæˆAPIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã¨ã—ã¦ã€
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç”»åƒã‚’è§£æã—ã€20ç§’é–“ï¼ˆ5ç§’Ã—4ã‚·ãƒ¼ãƒ³ï¼‰ã®æ§‹æˆæ¡ˆã‚’ä½œæˆã€‚

    Args:
        image_url: åˆ†æå¯¾è±¡ã®ç”»åƒURL
        mood: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãŸãƒ†ãƒ¼ãƒ/ãƒ ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼šæ¥½ã—ã„ã€æ„Ÿå‹•ã€ãƒ­ãƒãƒ³ãƒãƒƒã‚¯ï¼‰
        video_provider: å‹•ç”»ç”Ÿæˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆ"runway", "veo", "domoai", "piapi_kling"ï¼‰

    Returns:
        dict: {
            "title": str,
            "theme": str,
            "scenes": [
                {
                    "scene_number": int,
                    "act": str,  # èµ·/æ‰¿/è»¢/çµ
                    "description_ja": str,
                    "runway_prompt": str,
                    "camera_work": str,
                    "mood": str,
                    "duration_seconds": int
                }
            ]
        }
    """
    client = get_gemini_client()

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
    template = load_prompt_template(video_provider)
    provider_name = "Google Veo 2" if video_provider == "veo" else "Runway Gen-3 Alpha"

    # ãƒ ãƒ¼ãƒ‰æŒ‡å®šãŒã‚ã‚‹å ´åˆã®è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    mood_instruction = ""
    if mood:
        mood_instruction = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ­ USER'S REQUESTED MOOD/THEME: {mood}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CRITICAL: The user wants a "{mood}" style video. You MUST:
- Match the overall tone and atmosphere to this mood
- Choose camera movements that enhance this feeling
- Write prompts that evoke this emotional quality
- Ensure the 4-scene arc builds toward and resolves this mood

Mood-specific guidelines:
- æ¥½ã—ã„/ãƒãƒƒãƒ— (Happy/Pop): Bright colors, dynamic movement, upbeat energy, smiles
- æ„Ÿå‹•/åˆ‡ãªã„ (Emotional/Sad): Soft lighting, gentle movements, contemplative moments
- ãƒ­ãƒãƒ³ãƒãƒƒã‚¯ (Romantic): Warm tones, intimate framing, soft focus, tender expressions
- ã‚¨ãƒãƒ«ã‚®ãƒƒã‚·ãƒ¥ (Energetic): Fast camera moves, dynamic action, high contrast
- ç©ã‚„ã‹/ç™’ã— (Calm/Healing): Slow movements, natural lighting, peaceful scenes
- Custom mood: Interpret and apply the user's specific request creatively

"""

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ„ã¿è¾¼ã¿
    template_instruction = ""
    if template["reference_rule"] or template["clip_specific_template"]:
        template_instruction = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ PROMPT TEMPLATE STRUCTURE (FOLLOW THIS FORMAT)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{template["reference_rule"]}

{template["clip_specific_template"]}
"""
        if template["negative_prompt"] and video_provider == "runway":
            template_instruction += f"""
NEGATIVE PROMPT (include in output for Runway):
{template["negative_prompt"]}
"""

    # ä¸–ç•Œæœ€é«˜å³°ã®æ˜ ç”»ç›£ç£ã¨ã—ã¦ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    system_prompt = mood_instruction + template_instruction + f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¬ DIRECTOR'S MODE: WORLD-CLASS FILM DIRECTOR & {provider_name.upper()} EXPERT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You are a world-renowned film director and {provider_name} prompt engineering master.
Your mission: Transform this single image into a compelling 20-second short film (4 scenes Ã— 5 seconds).

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“œ THE FOUR-ACT STRUCTURE (èµ·æ‰¿è»¢çµ - KishÅtenketsu)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ã€èµ· (KI) - INTRODUCTIONã€‘Scene 1
- Establish the world, subject, and atmosphere
- Camera: SLOW, contemplative (Slow Zoom In, Static Wide, Gentle Pan)
- Movement: Minimal, breathing, subtle environmental motion
- Purpose: Draw viewer into the frame, create intimacy

ã€æ‰¿ (SHÅŒ) - DEVELOPMENTã€‘Scene 2
- Build upon the introduction, add narrative momentum
- Camera: TRACKING, following movement (Dolly, Arc Shot, Push In)
- Movement: Subject begins to move, interact, or respond
- Purpose: Deepen engagement, hint at what's to come

ã€è»¢ (TEN) - TWIST/CLIMAXã€‘Scene 3
- The pivotal moment, peak of visual/emotional intensity
- Camera: DYNAMIC, impactful (Whip Pan, Quick Zoom, Dutch Angle, Crane)
- Movement: Dramatic gesture, revelation, peak action
- Purpose: Create the "wow" moment, maximum visual impact

ã€çµ (KETSU) - CONCLUSIONã€‘Scene 4
- Resolution, emotional landing, lingering impression
- Camera: SLOW, reflective (Slow Zoom Out, Wide, Fade)
- Movement: Return to stillness, contemplative pose, environmental response
- Purpose: Leave lasting impression, invite rewatching

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ {provider_name.upper()} OPTIMIZATION RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ã€CRITICAL: Follow the PROMPT TEMPLATE STRUCTURE above for runway_prompt outputã€‘

ã€MUST INCLUDE in every promptã€‘
- REFERENCE RULE section (for image reference continuity)
- CLIP SPECIFIC section with: Scene, Subject, Micro-expression, Camera, Lighting, Must include, Final note
- Subject description with FIXED attributes (same person/object across all scenes)
- "Same [subject] from previous scene" for scenes 2-4

ã€AVOIDã€‘
- Abstract concepts without visual anchor
- Multiple scene changes within one prompt
- Conflicting motion directions
- Overcomplicated scenarios
- PHYSICALLY IMPOSSIBLE MOVEMENTS (see constraints below)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ PHYSICAL CONSTRAINTS - MANDATORY (Even for Animation)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ã€HUMAN BODY RANGE OF MOTION - STRICTLY ENFORCEã€‘
- HEAD/NECK: Maximum rotation ~80Â° left/right (NOT 360Â°!), tilt ~45Â° side, ~45Â° forward/back
- SHOULDERS: Maximum rotation ~180Â° forward, ~60Â° backward
- ELBOWS: Bend 0Â°-145Â° only, NO hyperextension, NO backward bending
- WRISTS: Rotation ~180Â° total, flex ~80Â°, extend ~70Â°
- SPINE: Gradual curves only, NO sudden 90Â° bends, NO impossible twists
- HIPS: ~120Â° flexion, ~30Â° extension, ~45Â° abduction
- KNEES: Bend 0Â°-140Â° only, NO backward bending

ã€FORBIDDEN MOVEMENTS - NEVER GENERATEã€‘
âŒ Head rotating 360Â° or more than 90Â° in either direction
âŒ Limbs bending in wrong direction (knees bending forward, elbows bending backward)
âŒ Spine twisting unnaturally or folding at sharp angles
âŒ Fingers bending backward or in impossible directions
âŒ Body parts detaching, stretching unnaturally, or passing through each other
âŒ Instantaneous teleportation or position changes without transition
âŒ Floating or defying gravity without clear artistic intent

ã€BODY PART PERSISTENCE - CRITICALã€‘
All body parts must remain visible and consistent throughout the entire video:
- FINGERS: All 5 fingers on each hand must be visible when hands are shown. Never let fingers disappear, merge, or change count mid-animation.
- HANDS: Both hands must remain attached and visible. If one hand is shown, it must stay visible or naturally move out of frame (not vanish).
- LIMBS: Arms and legs must not disappear or phase in/out during movement.
- FACIAL FEATURES: Eyes, nose, mouth, ears must remain consistent. No morphing or disappearing.
- HAIR: Hair volume and style must remain consistent (no sudden bald patches or style changes).

ã€PROMPT KEYWORDS FOR BODY PERSISTENCEã€‘
Always include these phrases when hands/fingers are involved:
âœ“ "maintaining all five fingers visible"
âœ“ "hands remain fully formed throughout"
âœ“ "consistent body structure"
âœ“ "no missing or morphing body parts"

ã€EVEN FOR STYLIZED ANIMATIONã€‘
- Exaggerated movements are OK, but must respect basic joint mechanics
- Squash and stretch is OK, but body structure must remain coherent
- Fast movements are OK, but must have proper anticipation and follow-through
- Always maintain skeletal integrity - bones don't bend, joints have limits

ã€SAFE MOTION KEYWORDSã€‘
âœ“ "natural head turn", "gentle nod", "subtle glance"
âœ“ "smooth arm raise", "natural gesture", "relaxed pose shift"
âœ“ "realistic walk cycle", "believable movement", "anatomically correct motion"

ã€STYLE KEYWORDS that work wellã€‘
- cinematic, film grain, shallow depth of field
- natural lighting, golden hour, blue hour
- photorealistic, high detail, 4K quality
- smooth motion, fluid movement
- dramatic, intimate, contemplative

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¨ SUBJECT CONSISTENCY - CRITICAL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For HUMAN subjects, lock these attributes across ALL 4 scenes:
- Exact clothing colors and style
- Hair color, length, and style
- Accessories (jewelry, glasses, bags)
- Approximate age and build
- Skin tone

For OBJECT/ANIMAL subjects:
- Exact colors and patterns
- Size and proportions
- Distinctive features
- Material/texture

Include phrase: "maintaining exact appearance from scene 1" in scenes 2-4

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¤ OUTPUT FORMAT (JSON)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{{
  "title": "Short evocative Japanese title (2-6 characters)",
  "theme": "One-line theme in Japanese",
  "scenes": [
    {{
      "scene_number": 1,
      "act": "èµ·",
      "description_ja": "Japanese description for user (1-2 sentences, natural)",
      "runway_prompt": "English prompt optimized for {provider_name} (50-100 words)",
      "camera_work": "slow_zoom_in|tracking|dynamic_pan|slow_zoom_out|static|arc_shot|dolly_in|crane_up|whip_pan",
      "mood": "calm|building|intense|reflective|mysterious|joyful|melancholic",
      "duration_seconds": 5
    }},
    // ... scenes 2, 3, 4
  ]
}}

Output ONLY valid JSON. No explanations, no markdown code blocks.
"""

    try:
        # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        async with httpx.AsyncClient() as http_client:
            img_response = await http_client.get(image_url, timeout=30.0)
            img_response.raise_for_status()
            image_data = img_response.content

        # Geminiã«é€ä¿¡
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=[
                types.Part.from_bytes(data=image_data, mime_type="image/jpeg"),
                system_prompt
            ],
            config=types.GenerateContentConfig(temperature=0.8)
        )

        # JSONã‚’ãƒ‘ãƒ¼ã‚¹
        result_text = response.text.strip()
        # ```json ... ``` ã‚’é™¤å»
        if result_text.startswith("```"):
            lines = result_text.split("\n")
            result_text = "\n".join(lines[1:-1])

        storyboard = json.loads(result_text)

        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if "scenes" not in storyboard or len(storyboard["scenes"]) != 4:
            raise ValueError(f"Expected 4 scenes, got {len(storyboard.get('scenes', []))}")

        return storyboard

    except Exception as e:
        logger.exception(f"4-scene storyboard generation failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒœãƒ¼ãƒ‰ã‚’è¿”ã™
        return {
            "title": "ç‰©èª",
            "theme": "é™ã‹ãªç¬é–“ã®ç¾ã—ã•",
            "scenes": [
                {
                    "scene_number": 1,
                    "act": "èµ·",
                    "description_ja": "é™ã‹ã«ä½‡ã‚€å§¿ã€‚ã‚«ãƒ¡ãƒ©ãŒã‚†ã£ãã‚Šã¨è¿‘ã¥ã„ã¦ã„ãã€‚",
                    "runway_prompt": "A person standing still, peaceful expression, soft natural lighting, camera slowly zooms in, cinematic, photorealistic, 5 seconds",
                    "camera_work": "slow_zoom_in",
                    "mood": "calm",
                    "duration_seconds": 5
                },
                {
                    "scene_number": 2,
                    "act": "æ‰¿",
                    "description_ja": "ã‚ãšã‹ã«å‹•ãå§‹ã‚ã‚‹ã€‚è¦–ç·šãŒå‹•ãã€ä½•ã‹ã«æ°—ã¥ã„ãŸã‚ˆã†ãªè¡¨æƒ…ã€‚",
                    "runway_prompt": "Same person from scene 1, slight head turn, eyes looking to the side, subtle movement, tracking shot, cinematic, 5 seconds",
                    "camera_work": "tracking",
                    "mood": "building",
                    "duration_seconds": 5
                },
                {
                    "scene_number": 3,
                    "act": "è»¢",
                    "description_ja": "è¡¨æƒ…ãŒå¤‰ã‚ã‚‹ç¬é–“ã€‚æ„Ÿæƒ…ãŒè¡¨ã«å‡ºã‚‹ã€‚",
                    "runway_prompt": "Same person from scene 1, emotional expression change, dynamic camera movement, dramatic lighting, cinematic moment, 5 seconds",
                    "camera_work": "dynamic_pan",
                    "mood": "intense",
                    "duration_seconds": 5
                },
                {
                    "scene_number": 4,
                    "act": "çµ",
                    "description_ja": "ç©ã‚„ã‹ãªè¡¨æƒ…ã«æˆ»ã‚Šã€ã‚«ãƒ¡ãƒ©ãŒå¼•ã„ã¦ã„ãã€‚ä½™éŸ»ã‚’æ®‹ã™ã€‚",
                    "runway_prompt": "Same person from scene 1, peaceful resolution, soft smile, camera slowly zooms out, lingering shot, cinematic, 5 seconds",
                    "camera_work": "slow_zoom_out",
                    "mood": "reflective",
                    "duration_seconds": 5
                }
            ]
        }


async def generate_story_frame_image(
    prompt: str,
    reference_image_url: str | None = None,
    previous_scene_image_url: str | None = None,
    aspect_ratio: str = "9:16",
) -> bytes | None:
    """
    å…ƒç”»åƒã¨ç›´å‰ã®ã‚·ãƒ¼ãƒ³ç”»åƒã‚’å‚ç…§ã—ãªãŒã‚‰ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ã„ã¦æ–°ã—ã„ç”»åƒã‚’ç”Ÿæˆ
    ï¼ˆå³æ ¼ãªIdentity Preservationä»˜ãï¼‰

    Args:
        prompt: ç”»åƒç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        reference_image_url: å‚ç…§å…ƒã®ç”»åƒURLï¼ˆä¸»å½¹ã®ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ï¼‰
        previous_scene_image_url: ç›´å‰ã®ã‚·ãƒ¼ãƒ³ã®ç”»åƒURLï¼ˆã‚·ãƒ¼ãƒ³é–“ã®é€£ç¶šæ€§ã‚’ä¿ã¤ãŸã‚ï¼‰
        aspect_ratio: ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: "9:16" ç¸¦é•·ãƒãƒ¼ãƒˆãƒ¬ãƒ¼ãƒˆï¼‰

    Returns:
        bytes: ç”Ÿæˆã•ã‚ŒãŸç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã€å¤±æ•—æ™‚ã¯None
    """
    client = get_gemini_client()

    try:
        contents = []

        # å…ƒç”»åƒãŒã‚ã‚‹å ´åˆã¯å‚ç…§ã¨ã—ã¦æ¸¡ã™ï¼ˆImage 1: Identity Anchorï¼‰
        if reference_image_url:
            async with httpx.AsyncClient() as http_client:
                img_response = await http_client.get(reference_image_url, timeout=30.0)
                img_response.raise_for_status()
                image_data = img_response.content

            contents.append(types.Part.from_bytes(data=image_data, mime_type="image/jpeg"))

        # ç›´å‰ã®ã‚·ãƒ¼ãƒ³ç”»åƒãŒã‚ã‚‹å ´åˆã‚‚å‚ç…§ã¨ã—ã¦æ¸¡ã™ï¼ˆImage 2: Previous Sceneï¼‰
        if previous_scene_image_url:
            async with httpx.AsyncClient() as http_client:
                img_response = await http_client.get(previous_scene_image_url, timeout=30.0)
                img_response.raise_for_status()
                prev_image_data = img_response.content

            contents.append(types.Part.from_bytes(data=prev_image_data, mime_type="image/jpeg"))

            # 2æšã®å‚ç…§ç”»åƒãŒã‚ã‚‹å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            if previous_scene_image_url:
                enhanced_prompt = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ DUAL-REFERENCE IDENTITY PRESERVATION MODE âš ï¸
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REFERENCE IMAGES PROVIDED:
- IMAGE 1 (First): Original/Source image - The IDENTITY ANCHOR (ground truth for appearance)
- IMAGE 2 (Second): Previous scene image - The CONTINUITY REFERENCE (for scene flow)

CRITICAL IMAGE FORMAT REQUIREMENT:
- Aspect ratio: {aspect_ratio} (portrait/vertical orientation)
- Generate a TALL, VERTICAL image suitable for short-form video (TikTok/Reels style)
- Width should be LESS than height (portrait mode)

YOUR TASK:
Generate the NEXT SCENE in this video sequence that:
1. Preserves EXACT identity from IMAGE 1 (original)
2. Follows naturally from IMAGE 2 (previous scene)
3. Applies the new prompt instructions below

ğŸ”’ IDENTITY PRESERVATION (From IMAGE 1 - IMMUTABLE):
- Face structure, features, all facial details
- Eye color, shape, characteristics
- Hair color, length, style, texture
- Skin tone and distinctive marks
- Clothing colors, patterns, materials
- All accessories

ğŸ”„ CONTINUITY (From IMAGE 2 - Flow naturally from this):
- Overall pose progression should feel natural
- Lighting consistency
- Emotional arc continuity
- Camera perspective similarity

âœ… WHAT CAN CHANGE (Per the prompt below):
- Facial expression
- Eye direction
- Head angle (slight)
- Body pose
- Natural movement

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GENERATION PROMPT:
{prompt}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Generate an image that seamlessly continues from the previous scene.
A viewer must immediately recognize this as the SAME PERSON progressing through a story.
"""
            else:
                # å…ƒç”»åƒã®ã¿ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                enhanced_prompt = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ STRICT IDENTITY PRESERVATION MODE âš ï¸
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CRITICAL IMAGE FORMAT REQUIREMENT:
- Aspect ratio: {aspect_ratio} (portrait/vertical orientation)
- Generate a TALL, VERTICAL image suitable for short-form video (TikTok/Reels style)
- Width should be LESS than height (portrait mode)

You are generating the NEXT FRAME in a video sequence.
The reference image above is the GROUND TRUTH.
Your generated image MUST be indistinguishable in identity from the reference.

ğŸ”’ ABSOLUTE REQUIREMENTS (FAILURE IF VIOLATED):

1. FACE IDENTITY - EXACT MATCH REQUIRED:
   - Same person/character - no exceptions
   - Identical facial structure, features, proportions
   - Same eye color, shape, and characteristics
   - Same nose, mouth, and facial details
   - Same skin tone and any distinctive marks

2. HAIR - EXACT MATCH REQUIRED:
   - Same color (including any highlights)
   - Same length and style
   - Same texture and arrangement

3. BODY - EXACT MATCH REQUIRED:
   - Same body type and proportions
   - Same skin tone on visible areas

4. CLOTHING & ACCESSORIES - EXACT MATCH REQUIRED:
   - Every piece of clothing must be identical
   - Same colors, patterns, and materials
   - All accessories in same position

5. BACKGROUND - EXACT MATCH REQUIRED:
   - Same location and environment
   - Same objects and atmosphere
   - Same lighting direction

âœ… ONLY THESE CAN CHANGE:
- Facial expression
- Eye direction
- Head angle (slight)
- Body pose
- Natural blink

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GENERATION PROMPT:
{prompt}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Generate an image that is the NEXT VIDEO FRAME.
A viewer must immediately recognize this as the SAME PERSON in the SAME SCENE.
If you cannot preserve identity exactly, DO NOT generate - this is CRITICAL.
"""
            contents.append(enhanced_prompt)
        else:
            # å‚ç…§ç”»åƒãªã—ã®å ´åˆã‚‚ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’æŒ‡å®š
            aspect_prompt = f"""
CRITICAL IMAGE FORMAT REQUIREMENT:
- Aspect ratio: {aspect_ratio} (portrait/vertical orientation)
- Generate a TALL, VERTICAL image suitable for short-form video (TikTok/Reels style)
- Width should be LESS than height (portrait mode)

{prompt}
"""
            contents.append(aspect_prompt)

        response = client.models.generate_content(
            model="gemini-2.0-flash-exp",
            contents=contents,
            config=types.GenerateContentConfig(
                response_modalities=["image", "text"],
            )
        )

        for part in response.parts:
            if part.inline_data:
                return part.inline_data.data

        return None

    except Exception as e:
        logger.exception(f"Story frame image generation failed: {e}")
        return None


async def translate_scene_to_runway_prompt(
    description_ja: str,
    scene_number: int,
    base_image_context: str | None = None,
    video_provider: str = "runway",
    scene_act: str | None = None,
    template_mode: str = "story",
    subject_type: str | None = None,
    camera_work: str | None = None,
    animation_category: str | None = None,
    animation_template: str | None = None,
) -> str:
    """
    æ—¥æœ¬èªã®ã‚·ãƒ¼ãƒ³èª¬æ˜ã‚’APIç”¨ã®è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹é€ ã‚’ç¶­æŒï¼‰

    Args:
        description_ja: æ—¥æœ¬èªã®ã‚·ãƒ¼ãƒ³èª¬æ˜
        scene_number: ã‚·ãƒ¼ãƒ³ç•ªå·ï¼ˆ1-16ã€ã‚µãƒ–ã‚·ãƒ¼ãƒ³å«ã‚€ï¼‰
        base_image_context: å…ƒç”»åƒã®èª¬æ˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        video_provider: å‹•ç”»ç”Ÿæˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆ"runway", "veo", "domoai", "piapi_kling"ï¼‰
        scene_act: ã‚·ãƒ¼ãƒ³ã®actï¼ˆèµ·/æ‰¿/è»¢/çµï¼‰- ã‚µãƒ–ã‚·ãƒ¼ãƒ³ã®å ´åˆã¯DBã‹ã‚‰å–å¾—
        template_mode: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆ"story"=é€£ç¶šæ€§é‡è¦–, "scene"=ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆé‡è¦–ï¼‰
        subject_type: è¢«å†™ä½“ã‚¿ã‚¤ãƒ—ï¼ˆ"person"/"object"/"animation"ï¼‰- sceneãƒ¢ãƒ¼ãƒ‰æ™‚ã®ã¿ä½¿ç”¨
        camera_work: ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠã®ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯ï¼ˆä¾‹: "slow zoom in", "pan left"ï¼‰
        animation_category: ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚«ãƒ†ã‚´ãƒªï¼ˆ"2d"/"3d"ï¼‰- animationé¸æŠæ™‚ã®ã¿
        animation_template: ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆIDï¼ˆ"A-1"ã€œ"B-4"ï¼‰- animationé¸æŠæ™‚ã®ã¿

    Returns:
        str: APIç”¨ã®è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹é€ ä»˜ãï¼‰
    """
    client = get_gemini_client()

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ¢ãƒ¼ãƒ‰ã¨è¢«å†™ä½“ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ç•°ãªã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ï¼‰
    template = load_prompt_template(
        video_provider,
        mode=template_mode,
        subject_type=subject_type,
        animation_category=animation_category,
        animation_template=animation_template
    )
    provider_name = "Google Veo 2" if video_provider == "veo" else "Runway Gen-3 Alpha"

    # actã‹ã‚‰é©åˆ‡ãªåå‰ã‚’æ±ºå®šï¼ˆã‚µãƒ–ã‚·ãƒ¼ãƒ³ã‚‚ã‚µãƒãƒ¼ãƒˆï¼‰
    act_to_name = {
        "èµ·": "èµ· (Introduction)",
        "æ‰¿": "æ‰¿ (Development)",
        "è»¢": "è»¢ (Climax)",
        "çµ": "çµ (Conclusion)",
    }
    scene_number_to_act = {1: "èµ·", 2: "æ‰¿", 3: "è»¢", 4: "çµ"}

    if scene_act:
        # DBã‹ã‚‰å–å¾—ã—ãŸactã‚’ä½¿ç”¨ï¼ˆã‚µãƒ–ã‚·ãƒ¼ãƒ³å¯¾å¿œï¼‰
        act_name = act_to_name.get(scene_act, f"Scene {scene_number}")
    else:
        # å¾“æ¥ã®æ–¹æ³•ï¼ˆscene_number 1-4ã®å ´åˆï¼‰
        act = scene_number_to_act.get(scene_number)
        act_name = act_to_name.get(act, f"Scene {scene_number}") if act else f"Scene {scene_number}"

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹é€ ã®èª¬æ˜
    template_structure = template.get("clip_specific_template", "")

    # ã‚µãƒ–ã‚·ãƒ¼ãƒ³ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    is_sub_scene = scene_number > 4
    scene_label = f"Scene {scene_number} (continuation)" if is_sub_scene else f"Scene {scene_number} of 4"

    # ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯æŒ‡ç¤ºã‚’æ§‹ç¯‰
    camera_instruction = ""
    if camera_work:
        camera_instruction = f"""
IMPORTANT - USER SELECTED CAMERA WORK:
The user has explicitly selected this camera movement: "{camera_work}"
You MUST use this exact camera work in the Camera field. Do not override or change it.
"""

    # ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    if subject_type == "animation" and animation_template:
        style_keywords = template.get("style_keywords", "")
        quality_boosters = template.get("quality_boosters", "")

        # DomoAIç”¨ã®å“è³ªãƒ–ãƒ¼ã‚¹ã‚¿ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãã¡ã‚‰ã‚’å„ªå…ˆ
        motion_quality = quality_boosters if quality_boosters else style_keywords
        if not motion_quality:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å“è³ªãƒ–ãƒ¼ã‚¹ã‚¿ãƒ¼
            motion_quality = "smooth fluid motion, high quality animation"

        # ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯æŒ‡ç¤ºï¼ˆstaticã®å ´åˆã‚‚è¢«å†™ä½“ã¯å‹•ãï¼‰
        if camera_work and camera_work.lower() in ["static", "static shot"]:
            camera_prompt = "static camera"
        elif camera_work:
            camera_prompt = camera_work
        else:
            camera_prompt = "slow gentle push in"

        system_prompt = f"""
You are an expert prompt engineer for {provider_name} Image-to-Video generation.

Convert the Japanese description into a CONCISE English prompt for animation-style video.

Japanese: {description_ja}
Animation Style: {animation_template}
Provider: {video_provider}

CRITICAL RULES (Image-to-Video):
- The image already shows: character design, background, colors
- DO NOT describe appearance, clothing, hair color, setting
- ONLY describe: action, motion, camera movement
- SUBJECT MUST ALWAYS ANIMATE (breathing, blinking, subtle movements)

QUALITY BOOSTERS (MUST include at end):
{motion_quality}

OUTPUT FORMAT (follow exactly):
[Atmosphere/mood]. [Action description]. [ANIMATED details: expression, eyes blinking, hair swaying, breathing]. {camera_prompt}. {motion_quality}.

CRITICAL - ANIMATION REQUIREMENTS:
- NEVER say "remains still" or "no movement"
- ALWAYS include at least 3 types of motion: facial expression + body movement + hair/clothing
- Motion words to use: "slowly", "gently", "subtly shifts", "gradually", "breathing rhythm"
- Even for calm scenes, include: natural breathing, slow blinks, micro-expressions
- ALWAYS end with the quality boosters: {motion_quality}

EXAMPLE OUTPUT:
Mischievous mood. Subtle smirk spreading, head tilting slightly. Eyes blinking innocently, gentle breathing rhythm visible, hair swaying softly. {camera_prompt}. {motion_quality}.

Return ONLY the prompt text. No quotes, no explanations. Do not end with extra periods.
"""
    else:
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆperson/objectï¼‰ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = f"""
You are an expert prompt engineer for {provider_name} video generation API.

Convert the following Japanese scene description into an optimized English prompt
that follows the EXACT template structure below.

Scene: {act_name} ({scene_label})
Japanese Description: {description_ja}
{camera_instruction}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REQUIRED OUTPUT TEMPLATE STRUCTURE (FOLLOW THIS EXACTLY):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{template.get("reference_rule", "")}

{template_structure}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RULES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Output MUST follow the CLIP SPECIFIC template structure above
2. Fill in each field (Scene, Subject, Micro-expression, Camera, Lighting, Must include, Final note)
3. Translate the Japanese meaning, not word-by-word
4. Keep subject identity consistent across scenes
5. For scenes 2-4, emphasize continuity with "Same [subject] from previous scene"
6. Add cinematic details appropriate for each field
7. If user selected a camera work, use it EXACTLY in the Camera field

OUTPUT FORMAT:
Return ONLY the structured prompt following the template above.
Start with "REFERENCE RULE" section, then "CLIP SPECIFIC" section.
No additional explanations or quotes.
"""

    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=system_prompt,
            config=types.GenerateContentConfig(temperature=0.7)
        )

        result = response.text.strip()
        # ä½™åˆ†ãªã‚¯ã‚©ãƒ¼ãƒˆã‚„æ”¹è¡Œã‚’å‰Šé™¤
        result = result.strip('"\'')
        return result

    except Exception as e:
        logger.exception(f"Translation failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹é€ 
        return f"""REFERENCE RULE (do not remove):
Use Image A as MASTER for identity, wardrobe, and world design.
{"Use Image B as PREVIOUS for continuity: exact outfit details, props, and layout." if scene_number > 1 else ""}
Keep the same person and same environment as a continuous scene.

CLIP SPECIFIC (edit only this block):
Scene: {description_ja}
Subject: Same subject maintaining appearance
Micro-expression: natural expression
Camera: cinematic framing, smooth movement
Lighting: natural lighting
Must include: continuity with previous scenes
Final note: Minimal motion, continuity over novelty, keep realism."""


async def generate_sub_scene_prompt(
    parent_prompt: str,
    parent_description_ja: str,
    sub_scene_order: int,
    camera_work: str | None = None,
    video_provider: str = "runway",
) -> dict:
    """
    è¦ªã‚·ãƒ¼ãƒ³ã‹ã‚‰é€£ç¶šæ€§ã®ã‚ã‚‹ã‚µãƒ–ã‚·ãƒ¼ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ

    è¦ªã‚·ãƒ¼ãƒ³ã®å‹•ç”»ã®ã€Œç¶šãã€ã¨ã—ã¦è‡ªç„¶ã«ç¹‹ãŒã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
    æ˜ ç”»ã®ã‚«ãƒƒãƒˆå‰²ã‚Šã®ã‚ˆã†ã«ã€åŒä¸€ã‚·ãƒ¼ãƒ³å†…ã§ã®åˆ¥ã‚¢ãƒ³ã‚°ãƒ«ã‚„å‹•ãã®ç¶™ç¶šã‚’è¡¨ç¾ã€‚

    Args:
        parent_prompt: è¦ªã‚·ãƒ¼ãƒ³ã®Runway/Veoãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        parent_description_ja: è¦ªã‚·ãƒ¼ãƒ³ã®æ—¥æœ¬èªèª¬æ˜
        sub_scene_order: ã‚µãƒ–ã‚·ãƒ¼ãƒ³é †åºï¼ˆ1, 2, 3ï¼‰
        camera_work: ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯æŒ‡å®šï¼ˆçœç•¥æ™‚ã¯è‡ªå‹•é¸æŠï¼‰
        video_provider: å‹•ç”»ç”Ÿæˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼

    Returns:
        {
            "description_ja": "æ—¥æœ¬èªèª¬æ˜",
            "runway_prompt": "è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆé€£ç¶šæ€§ä»˜ãï¼‰"
        }
    """
    client = get_gemini_client()

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
    template = load_prompt_template(video_provider)
    provider_name = "Google Veo 2" if video_provider == "veo" else "Runway Gen-3 Alpha"

    # ã‚µãƒ–ã‚·ãƒ¼ãƒ³ã®æ™‚é–“çš„ä½ç½®ã‚’è¡¨ç¾
    temporal_hints = {
        1: "moments later, continuing the action",
        2: "a beat later, the scene progresses",
        3: "following through, completing the motion",
    }
    temporal_hint = temporal_hints.get(sub_scene_order, "continuing seamlessly")

    system_prompt = f"""
You are an expert cinematographer creating seamless scene continuations for {provider_name}.

Given a parent scene, generate a natural CONTINUATION that feels like the next cut in a film.
This is sub-scene #{sub_scene_order} after the parent scene.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARENT SCENE (Reference - DO NOT repeat, but CONTINUE from this):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Japanese Description: {parent_description_ja}

Runway Prompt:
{parent_prompt}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CONTINUATION REQUIREMENTS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Temporal Flow: {temporal_hint}
2. Camera Work: {camera_work if camera_work else "Select appropriate continuation camera movement"}
3. Visual Continuity: Same subject, same lighting, same environment
4. Motion Continuity: Continue or complete the motion from parent scene
5. Do NOT restart the action - this is a continuation, not a new scene

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TEMPLATE STRUCTURE TO FOLLOW:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{template.get("clip_specific_template", "")}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OUTPUT FORMAT (JSON):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Return ONLY valid JSON with these two fields:
{{
    "description_ja": "Japanese description (1-2 sentences describing the continuation)",
    "runway_prompt": "Full prompt following the template structure above, with [Seamless continuation] prefix"
}}

IMPORTANT:
- The runway_prompt MUST start with "[Seamless continuation from previous cut]"
- Keep the same subject identity: "Same [subject] continuing..."
- Reference the previous frame: "Following the previous motion..."
- Do NOT use quotes around the JSON keys/values that would break parsing
"""

    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=system_prompt,
            config=types.GenerateContentConfig(temperature=0.7)
        )

        result_text = response.text.strip()

        # JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        import json
        import re

        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
        result_text = re.sub(r'^```json\s*', '', result_text)
        result_text = re.sub(r'\s*```$', '', result_text)
        result_text = result_text.strip()

        try:
            result = json.loads(result_text)
            return {
                "description_ja": result.get("description_ja", f"ï¼ˆ{parent_description_ja}ã®ç¶šãï¼‰"),
                "runway_prompt": result.get("runway_prompt", f"[Seamless continuation] {parent_prompt}"),
            }
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse JSON, using fallback: {result_text[:200]}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return {
                "description_ja": f"ï¼ˆ{parent_description_ja}ã®ç¶šã - ã‚«ãƒƒãƒˆ{sub_scene_order}ï¼‰",
                "runway_prompt": f"""[Seamless continuation from previous cut]

REFERENCE RULE (do not remove):
Use previous frame as MASTER for identity, wardrobe, and world design.
Continue the exact same scene - same person, same environment, same moment.

CLIP SPECIFIC (edit only this block):
Scene: Continuation of previous action, {temporal_hint}
Subject: Same subject from previous frame, continuing motion
Camera: {camera_work if camera_work else "Smooth continuation"}
Lighting: Maintain consistent lighting from previous cut
Must include: Visual continuity, motion flow from previous frame
Final note: Seamless transition, minimal jarring changes, maintain realism.""",
            }

    except Exception as e:
        logger.exception(f"Sub-scene prompt generation failed: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return {
            "description_ja": f"ï¼ˆ{parent_description_ja}ã®ç¶šãï¼‰",
            "runway_prompt": f"""[Seamless continuation from previous cut]

REFERENCE RULE (do not remove):
Continue from the previous frame. Same subject, same environment.

CLIP SPECIFIC:
Scene: Continuation - {temporal_hint}
Subject: Same subject continuing the action
Camera: {camera_work if camera_work else "Smooth hold or gentle movement"}
Lighting: Consistent with previous
Must include: Motion continuity
Final note: Seamless, natural progression.""",
        }


async def generate_ad_script(
    description: str,
    target_duration: int | None = None,
    aspect_ratio: str = "9:16"
) -> dict:
    """
    åºƒå‘Šã®èª¬æ˜ã‹ã‚‰CMæ§‹æˆï¼ˆã‚«ãƒƒãƒˆå‰²ã‚Šï¼‰ã‚’ç”Ÿæˆ

    åºƒå‘Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦ã€é©åˆ‡ãªåºƒå‘Šç†è«–ã‚’é¸æŠã—ã€
    åŠ¹æœçš„ãªã‚«ãƒƒãƒˆæ§‹æˆã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã€‚

    Args:
        description: åºƒå‘Šã®å†…å®¹ï¼ˆã©ã‚“ãªåºƒå‘Šã‚’ä½œã‚ŠãŸã„ã‹ï¼‰
        target_duration: å¸Œæœ›ã®å°ºï¼ˆç§’ï¼‰ã€‚15, 30, 60 ã¾ãŸã¯ Noneï¼ˆãŠã¾ã‹ã›ï¼‰
        aspect_ratio: ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼ˆ"9:16" ã¾ãŸã¯ "16:9"ï¼‰

    Returns:
        dict: {
            "id": str,
            "theory": str,
            "theory_label": str,
            "total_duration": int,
            "cuts": [
                {
                    "id": str,
                    "cut_number": int,
                    "scene_type": str,
                    "scene_type_label": str,
                    "description_ja": str,
                    "description_en": str,
                    "duration": int
                }
            ]
        }
    """
    import uuid

    client = get_gemini_client()

    # å°ºã®æŒ‡ç¤º
    duration_instruction = ""
    if target_duration:
        duration_instruction = f"""
ç›®æ¨™å°º: {target_duration}ç§’
- åˆè¨ˆç§’æ•°ãŒ{target_duration}ç§’ã«è¿‘ããªã‚‹ã‚ˆã†ã«ã‚«ãƒƒãƒˆæ§‹æˆã‚’èª¿æ•´ã—ã¦ãã ã•ã„
- å„ã‚«ãƒƒãƒˆã¯æœ€ä½2ç§’ã€æœ€å¤§10ç§’ã®ç¯„å›²ã§è¨­å®šã—ã¦ãã ã•ã„
"""
    else:
        duration_instruction = """
ç›®æ¨™å°º: ãŠã¾ã‹ã›ï¼ˆAIãŒæœ€é©ãªå°ºã‚’åˆ¤æ–­ï¼‰
- åºƒå‘Šå†…å®¹ã«å¿œã˜ã¦æœ€é©ãªå°ºã‚’æ±ºå®šã—ã¦ãã ã•ã„
- çŸ­ã„å•†å“ç´¹ä»‹ â†’ 15ç§’ç¨‹åº¦ï¼ˆ3-4ã‚«ãƒƒãƒˆï¼‰
- æ¨™æº–çš„ãªåºƒå‘Š â†’ 30ç§’ç¨‹åº¦ï¼ˆ4-6ã‚«ãƒƒãƒˆï¼‰
- ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã®ã‚ã‚‹åºƒå‘Š â†’ 45-60ç§’ç¨‹åº¦ï¼ˆ5-8ã‚«ãƒƒãƒˆï¼‰
"""

    system_prompt = f"""
ã‚ãªãŸã¯ä¸–ç•Œãƒˆãƒƒãƒ—ã‚¯ãƒ©ã‚¹ã®åºƒå‘Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸåºƒå‘Šã®èª¬æ˜ã‹ã‚‰ã€åŠ¹æœçš„ãªCMæ§‹æˆï¼ˆã‚«ãƒƒãƒˆå‰²ã‚Šï¼‰ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ åºƒå‘Šã®å†…å®¹
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{description}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â±ï¸ å°ºã®è¨­å®š
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{duration_instruction}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{aspect_ratio}ï¼ˆ{"ç¸¦é•·ãƒ»ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»å‘ã‘" if aspect_ratio == "9:16" else "æ¨ªé•·ãƒ»YouTubeç­‰å‘ã‘"}ï¼‰

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ åºƒå‘Šç†è«–ã®é¸æŠåŸºæº–
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ä»¥ä¸‹ã®ä¸­ã‹ã‚‰ã€åºƒå‘Šå†…å®¹ã«æœ€ã‚‚é©ã—ãŸç†è«–ã‚’1ã¤é¸æŠã—ã¦ãã ã•ã„ï¼š

ã€AIDAæ³•ã€‘aida - æ³¨ç›®â†’èˆˆå‘³â†’æ¬²æ±‚â†’è¡Œå‹•
- ã‚·ãƒ³ãƒ—ãƒ«ãªå•†å“ç´¹ä»‹å‘ã‘
- æ–°å•†å“ã®ãƒ­ãƒ¼ãƒ³ãƒã€æ©Ÿèƒ½è¨´æ±‚
- ã‚«ãƒƒãƒˆä¾‹: attentionï¼ˆæ³¨ç›®ï¼‰â†’ interestï¼ˆèˆˆå‘³ï¼‰â†’ desireï¼ˆæ¬²æ±‚ï¼‰â†’ actionï¼ˆè¡Œå‹•ï¼‰

ã€PASONAæ³•ã€‘pasona - å•é¡Œâ†’å…±æ„Ÿâ†’è§£æ±ºâ†’ææ¡ˆâ†’çµè¾¼â†’è¡Œå‹•
- èª²é¡Œè§£æ±ºå‹å•†å“å‘ã‘
- æ‚©ã¿è§£æ±ºã€ãƒ“ãƒ•ã‚©ãƒ¼ã‚¢ãƒ•ã‚¿ãƒ¼è¨´æ±‚
- ã‚«ãƒƒãƒˆä¾‹: problemï¼ˆå•é¡Œï¼‰â†’ affinityï¼ˆå…±æ„Ÿï¼‰â†’ solutionï¼ˆè§£æ±ºï¼‰â†’ offerï¼ˆææ¡ˆï¼‰â†’ narrowï¼ˆçµè¾¼ï¼‰â†’ actionï¼ˆè¡Œå‹•ï¼‰

ã€èµ·æ‰¿è»¢çµã€‘kishoutenketsu - å°å…¥â†’å±•é–‹â†’è»¢æ›â†’çµæœ«
- ã‚¹ãƒˆãƒ¼ãƒªãƒ¼é‡è¦–ã®åºƒå‘Šå‘ã‘
- ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã€æ„Ÿå‹•ç³»
- ã‚«ãƒƒãƒˆä¾‹: kiï¼ˆå°å…¥ï¼‰â†’ shoï¼ˆå±•é–‹ï¼‰â†’ tenï¼ˆè»¢æ›ï¼‰â†’ ketsuï¼ˆçµæœ«ï¼‰

ã€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°å‹ã€‘storytelling - ãƒ•ãƒƒã‚¯â†’èª²é¡Œâ†’æ—…â†’ç™ºè¦‹â†’å¤‰åŒ–â†’CTA
- æ„Ÿæƒ…è¨´æ±‚å‘ã‘
- ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ææ¡ˆã€ä½“é¨“å‹åºƒå‘Š
- ã‚«ãƒƒãƒˆä¾‹: hookï¼ˆãƒ•ãƒƒã‚¯ï¼‰â†’ challengeï¼ˆèª²é¡Œï¼‰â†’ journeyï¼ˆæ—…ï¼‰â†’ discoveryï¼ˆç™ºè¦‹ï¼‰â†’ transformationï¼ˆå¤‰åŒ–ï¼‰â†’ ctaï¼ˆè¡Œå‹•å–šèµ·ï¼‰

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¤ å‡ºåŠ›å½¢å¼ (JSON)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{{
  "theory": "aida|pasona|kishoutenketsu|storytelling",
  "theory_label": "AIDAæ³•ï¼ˆæ³¨ç›®â†’èˆˆå‘³â†’æ¬²æ±‚â†’è¡Œå‹•ï¼‰",
  "total_duration": 30,
  "cuts": [
    {{
      "cut_number": 1,
      "scene_type": "attention",
      "scene_type_label": "æ³¨ç›®",
      "description_ja": "å¿™ã—ã„æœã€æ™‚é–“ãŒãªãã¦æœé£Ÿã‚’æŠœã„ã¦ã—ã¾ã†å¥³æ€§",
      "description_en": "A busy woman rushing in the morning, skipping breakfast due to lack of time",
      "duration": 4
    }},
    // ... ä»¥ä¸‹åŒæ§˜
  ]
}}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. description_en ã¯å‹•ç”»ç”ŸæˆAIï¼ˆRunway/Veoï¼‰å‘ã‘ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦ä½¿ãˆã‚‹å½¢å¼ã§è¨˜è¿°
   - è¦–è¦šçš„ã«å…·ä½“çš„ãªæå†™ã‚’å«ã‚ã‚‹
   - ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯ã®æŒ‡ç¤ºã‚’å«ã‚ã¦ã‚‚è‰¯ã„
   - 50-100èªç¨‹åº¦ã§è©³ç´°ã«

2. å„ã‚«ãƒƒãƒˆã®durationã¯å†…å®¹ã«å¿œã˜ã¦æœ€é©åŒ–
   - å°å…¥ã‚«ãƒƒãƒˆ: 2-4ç§’ï¼ˆçŸ­ã‚ã§å°è±¡çš„ã«ï¼‰
   - èª¬æ˜ã‚«ãƒƒãƒˆ: 4-6ç§’ï¼ˆç†è§£ã«å¿…è¦ãªæ™‚é–“ï¼‰
   - ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹: 4-8ç§’ï¼ˆã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’ä¸ãˆã‚‹æ™‚é–“ï¼‰
   - CTAã‚«ãƒƒãƒˆ: 2-4ç§’ï¼ˆè¡Œå‹•ã‚’ä¿ƒã™ï¼‰

3. ã‚«ãƒƒãƒˆæ•°ã¯å†…å®¹ã«å¿œã˜ã¦æœ€é©åŒ–ï¼ˆé€šå¸¸3-8ã‚«ãƒƒãƒˆï¼‰

4. scene_type_labelã¯å¿…ãšæ—¥æœ¬èªã§è¨˜è¿°

å‡ºåŠ›ã¯JSONã®ã¿ã€‚èª¬æ˜ã‚„å‰ç½®ãã¯ä¸è¦ã€‚
"""

    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=system_prompt,
            config=types.GenerateContentConfig(temperature=0.8)
        )

        result_text = response.text.strip()

        # ```json ... ``` ã‚’é™¤å»
        if result_text.startswith("```"):
            lines = result_text.split("\n")
            result_text = "\n".join(lines[1:-1])

        script_data = json.loads(result_text)

        # IDã‚’ç”Ÿæˆ
        script_id = f"script_{uuid.uuid4().hex[:12]}"

        # ã‚«ãƒƒãƒˆã«IDã‚’ä»˜ä¸
        cuts_with_ids = []
        for i, cut in enumerate(script_data.get("cuts", [])):
            cut_with_id = {
                "id": f"cut_{uuid.uuid4().hex[:8]}",
                "cut_number": cut.get("cut_number", i + 1),
                "scene_type": cut.get("scene_type", "unknown"),
                "scene_type_label": cut.get("scene_type_label", "ã‚·ãƒ¼ãƒ³"),
                "description_ja": cut.get("description_ja", ""),
                "description_en": cut.get("description_en", ""),
                "duration": cut.get("duration", 5),
            }
            cuts_with_ids.append(cut_with_id)

        # åˆè¨ˆç§’æ•°ã‚’è¨ˆç®—
        total_duration = sum(cut["duration"] for cut in cuts_with_ids)

        # theoryã‚’å°æ–‡å­—ã«æ­£è¦åŒ–ï¼ˆGeminiãŒå¤§æ–‡å­—ã§è¿”ã™å ´åˆãŒã‚ã‚‹ãŸã‚ï¼‰
        theory_raw = script_data.get("theory", "aida")
        theory = theory_raw.lower() if isinstance(theory_raw, str) else "aida"

        # æœ‰åŠ¹ãªç†è«–ã‹ãƒã‚§ãƒƒã‚¯
        valid_theories = ["aida", "pasona", "kishoutenketsu", "storytelling"]
        if theory not in valid_theories:
            theory = "aida"

        return {
            "id": script_id,
            "theory": theory,
            "theory_label": script_data.get("theory_label", "AIDAæ³•"),
            "total_duration": total_duration,
            "cuts": cuts_with_ids,
        }

    except Exception as e:
        logger.exception(f"Ad script generation failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªAIDAæ§‹æˆã‚’è¿”ã™
        import uuid
        script_id = f"script_{uuid.uuid4().hex[:12]}"
        fallback_duration = target_duration or 30

        return {
            "id": script_id,
            "theory": "aida",
            "theory_label": "AIDAæ³•ï¼ˆæ³¨ç›®â†’èˆˆå‘³â†’æ¬²æ±‚â†’è¡Œå‹•ï¼‰",
            "total_duration": fallback_duration,
            "cuts": [
                {
                    "id": f"cut_{uuid.uuid4().hex[:8]}",
                    "cut_number": 1,
                    "scene_type": "attention",
                    "scene_type_label": "æ³¨ç›®",
                    "description_ja": "è¦–è´è€…ã®æ³¨æ„ã‚’å¼•ãã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã®ã‚ã‚‹ã‚·ãƒ¼ãƒ³",
                    "description_en": "Eye-catching opening scene that grabs viewer attention, dynamic camera movement",
                    "duration": max(3, fallback_duration // 4),
                },
                {
                    "id": f"cut_{uuid.uuid4().hex[:8]}",
                    "cut_number": 2,
                    "scene_type": "interest",
                    "scene_type_label": "èˆˆå‘³",
                    "description_ja": "å•†å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã«èˆˆå‘³ã‚’æŒãŸã›ã‚‹ã‚·ãƒ¼ãƒ³",
                    "description_en": "Scene showcasing the product or service features, building curiosity",
                    "duration": max(4, fallback_duration // 4),
                },
                {
                    "id": f"cut_{uuid.uuid4().hex[:8]}",
                    "cut_number": 3,
                    "scene_type": "desire",
                    "scene_type_label": "æ¬²æ±‚",
                    "description_ja": "å•†å“ã‚’æ¬²ã—ã„ã¨æ€ã‚ã›ã‚‹ã‚·ãƒ¼ãƒ³",
                    "description_en": "Scene creating desire, showing benefits and positive outcomes",
                    "duration": max(4, fallback_duration // 4),
                },
                {
                    "id": f"cut_{uuid.uuid4().hex[:8]}",
                    "cut_number": 4,
                    "scene_type": "action",
                    "scene_type_label": "è¡Œå‹•",
                    "description_ja": "è¡Œå‹•ã‚’ä¿ƒã™CTAï¼ˆã‚³ãƒ¼ãƒ«ãƒ»ãƒˆã‚¥ãƒ»ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰",
                    "description_en": "Call to action with product display and purchase prompt",
                    "duration": max(3, fallback_duration - (fallback_duration // 4) * 3),
                },
            ],
        }



# ===== Text-to-Image ç¿»è¨³ãƒ»å¤‰æ›é–¢æ•° =====

# ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³å€¤ã®è‹±èªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆschemas.pyã¨åŒæœŸï¼‰
_POSITION_EN_MAP = {
    "center": "centered in frame",
    "left": "positioned left of center",
    "right": "positioned right of center",
    "upper": "positioned at upper third",
    "lower": "positioned at lower third",
    "rule_of_thirds": "following rule of thirds",
}

_LIGHTING_EN_MAP = {
    "soft_natural": "soft natural daylight",
    "dramatic": "dramatic directional lighting",
    "studio": "professional studio lighting",
    "backlit": "backlighting with rim highlights",
    "golden_hour": "warm golden hour lighting",
    "moody": "moody low-key lighting",
}

_MOOD_EN_MAP = {
    "luxury": "sophisticated luxury aesthetic",
    "energetic": "dynamic energetic feel",
    "calm": "calm serene atmosphere",
    "playful": "playful whimsical mood",
    "professional": "clean professional look",
    "nostalgic": "nostalgic vintage feel",
}


def _is_likely_english(text: str) -> bool:
    """
    ãƒ†ã‚­ã‚¹ãƒˆãŒè‹±èªã‹ã©ã†ã‹ã‚’ç°¡æ˜“åˆ¤å®š
    ASCIIæ¯”ç‡ãŒ80%ä»¥ä¸Šãªã‚‰è‹±èªã¨åˆ¤å®š
    """
    if not text:
        return True
    ascii_chars = sum(1 for c in text if ord(c) < 128)
    return ascii_chars / len(text) > 0.8


async def _translate_text_to_english(text: str) -> str:
    """
    æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’è‹±èªã«ç¿»è¨³
    """
    client = get_gemini_client()

    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=f"Translate the following Japanese text to English. Return ONLY the English translation, nothing else:\n\n{text}",
            config=types.GenerateContentConfig(temperature=0.3)
        )
        result = response.text.strip()
        # ã‚¯ã‚©ãƒ¼ãƒˆã‚’é™¤å»
        result = result.strip('"\'')
        return result
    except Exception as e:
        logger.warning(f"Translation failed, using original text: {e}")
        return text


async def translate_structured_input_to_english(structured_input: dict) -> dict:
    """
    æ§‹é€ åŒ–å…¥åŠ›ã®æ—¥æœ¬èªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è‹±èªã«ç¿»è¨³

    - ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆsubject, background, color_palette, additional_notesï¼‰ã¯ç¿»è¨³
    - ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³å€¤ï¼ˆsubject_position, lighting, moodï¼‰ã¯äº‹å‰å®šç¾©ã®è‹±èªå€¤ã‚’ä½¿ç”¨
    - æ—¢ã«è‹±èªã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™

    Args:
        structured_input: æ§‹é€ åŒ–å…¥åŠ›è¾æ›¸

    Returns:
        dict: è‹±èªã«å¤‰æ›ã•ã‚ŒãŸæ§‹é€ åŒ–å…¥åŠ›
    """
    result = structured_input.copy()

    # ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³å€¤ã¯äº‹å‰å®šç¾©ã®è‹±èªã«å¤‰æ›
    if result.get("subject_position"):
        result["subject_position"] = _POSITION_EN_MAP.get(
            result["subject_position"],
            result["subject_position"]
        )

    if result.get("lighting"):
        result["lighting"] = _LIGHTING_EN_MAP.get(
            result["lighting"],
            result["lighting"]
        )

    if result.get("mood"):
        result["mood"] = _MOOD_EN_MAP.get(
            result["mood"],
            result["mood"]
        )

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ç¿»è¨³ï¼ˆæ—¥æœ¬èªã®å ´åˆã®ã¿ï¼‰
    text_fields = ["subject", "background", "color_palette", "additional_notes"]

    for field in text_fields:
        value = result.get(field)
        if value and not _is_likely_english(value):
            result[field] = await _translate_text_to_english(value)
            logger.info(f"Translated {field}: {value} -> {result[field]}")

    return result


async def generate_image_prompt_from_scene(
    description_ja: str | None,
    dialogue: str | None = None,
    aspect_ratio: str = "9:16",
    structured_input: dict | None = None,
    reference_image_url: str | None = None,
) -> tuple[str, str]:
    """
    è„šæœ¬ã¾ãŸã¯æ§‹é€ åŒ–å…¥åŠ›ã‹ã‚‰ç”»åƒç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ

    Args:
        description_ja: ã‚«ãƒƒãƒˆã®è„šæœ¬ï¼ˆæ—¥æœ¬èªï¼‰- å¾“æ¥ãƒ¢ãƒ¼ãƒ‰ç”¨
        dialogue: ã‚«ãƒƒãƒˆã®ã‚»ãƒªãƒ•ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰- å¾“æ¥ãƒ¢ãƒ¼ãƒ‰ç”¨
        aspect_ratio: ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”
        structured_input: æ§‹é€ åŒ–å…¥åŠ›ï¼ˆText-to-Imageç”¨ã€è‹±èªã«ç¿»è¨³æ¸ˆã¿ï¼‰
        reference_image_url: å‚ç…§ç”»åƒURLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ç”¨ï¼‰

    Returns:
        tuple[str, str]: (æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ, è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ)
    """
    client = get_gemini_client()

    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®èª¬æ˜
    aspect_desc = "ç¸¦é•·ï¼ˆ9:16ï¼‰" if aspect_ratio == "9:16" else "æ¨ªé•·ï¼ˆ16:9ï¼‰"

    # æ§‹é€ åŒ–å…¥åŠ›ãŒã‚ã‚‹å ´åˆã¯æ–°ã—ã„ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
    if structured_input:
        return await _generate_prompt_from_structured_input(
            client, structured_input, aspect_ratio, aspect_desc, reference_image_url
        )

    # å¾“æ¥ãƒ¢ãƒ¼ãƒ‰: description_ja + dialogue
    return await _generate_prompt_from_description(
        client, description_ja, dialogue, aspect_ratio, aspect_desc
    )


async def _generate_prompt_from_structured_input(
    client,
    structured_input: dict,
    aspect_ratio: str,
    aspect_desc: str,
    reference_image_url: str | None = None,
) -> tuple[str, str]:
    """
    æ§‹é€ åŒ–å…¥åŠ›ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆï¼ˆText-to-Imageç”¨ï¼‰
    å‚ç…§ç”»åƒãŒã‚ã‚‹å ´åˆã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã¨ã—ã¦æ¸¡ã™
    """
    # æ§‹é€ åŒ–å…¥åŠ›ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
    subject = structured_input.get("subject", "")
    subject_position = structured_input.get("subject_position", "centered in frame")
    background = structured_input.get("background", "")
    lighting = structured_input.get("lighting", "soft natural daylight")
    color_palette = structured_input.get("color_palette", "")
    mood = structured_input.get("mood", "")
    additional_notes = structured_input.get("additional_notes", "")

    # æ§‹é€ åŒ–å…¥åŠ›ã‚’æ•´å½¢
    input_parts = [f"Subject: {subject}"]
    if subject_position:
        input_parts.append(f"Position: {subject_position}")
    if background:
        input_parts.append(f"Background: {background}")
    if lighting:
        input_parts.append(f"Lighting: {lighting}")
    if color_palette:
        input_parts.append(f"Color palette: {color_palette}")
    if mood:
        input_parts.append(f"Mood: {mood}")
    if additional_notes:
        input_parts.append(f"Additional notes: {additional_notes}")

    input_text = "\n".join(input_parts)

    # ARRI ã‚«ãƒ¡ãƒ©ãƒ»ãƒ¬ãƒ³ã‚ºãƒ«ãƒƒã‚¯ã®å¿…é ˆãƒ•ãƒ¬ãƒ¼ã‚º
    arri_look_phrase = """Shot on ARRI ALEXA 35 with ARRI Signature Prime lens. ArriRaw to Rec709 color conversion applied, delivering cinematic skin tones, natural color rendition, and characteristic ARRI color science with smooth roll-off in highlights and rich shadow detail."""

    # å‚ç…§ç”»åƒã®æœ‰ç„¡ã§ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆ
    if reference_image_url:
        system_prompt = f"""
You are a professional advertising creative director specializing in luxury brand imagery.
Create high-quality prompts for generating scene images with ARRI cinematic look.

## MANDATORY: Camera & Lens Look
ALL generated prompts MUST include this camera specification:
{arri_look_phrase}

## IMPORTANT: Reference Image Analysis
You are provided with a REFERENCE IMAGE (referred to as [INPUT_IMAGE]).
The subject in this image is referred to as [INPUT_IMAGE_SUBJECT].

You MUST:
1. Carefully analyze [INPUT_IMAGE] for: subject, style, colors, lighting, composition, textures
2. Preserve the visual identity and aesthetic of [INPUT_IMAGE]
3. Use [INPUT_IMAGE_SUBJECT] as the hero element in the generated scene
4. Match the lighting direction and quality from [INPUT_IMAGE]

## Input Variables
- [INPUT_IMAGE]: The reference image you are analyzing
- [INPUT_IMAGE_SUBJECT]: The main subject extracted from the reference image

## Structured Input Fields
- Subject: Main subject description (use this to understand [INPUT_IMAGE_SUBJECT])
- Position: Subject's position in frame (e.g., "centered at 50% horizontal, 52% vertical")
- Background: Setting/environment to create around [INPUT_IMAGE_SUBJECT]
- Lighting: Light type and direction (must match [INPUT_IMAGE])
- Color palette: Main colors (incorporate colors from [INPUT_IMAGE])
- Mood: Emotional atmosphere
- Additional notes: Extra instructions

## Output: 5-Stage Nanobanana Structure

### 1. Main Concept (1-2 sentences)
"This image employs [technique] to create [environment] for [INPUT_IMAGE_SUBJECT]. Shot on ARRI ALEXA 35 with ARRI Signature Prime lens."
â†’ Describe the visual concept integrating [INPUT_IMAGE_SUBJECT]

### 2. Visual Signature (1 sentence)
"[Composition description] with [INPUT_IMAGE_SUBJECT] at [position], surrounded by [elements], [color/composition from INPUT_IMAGE]."
â†’ Incorporate colors, textures, and composition from [INPUT_IMAGE]

### 3. Technical Approach (1-2 sentences)
"[Photography type] with [techniques], matching the lighting of [INPUT_IMAGE]. ArriRaw to Rec709 color conversion applied."
â†’ Match the photographic style of [INPUT_IMAGE]

### 4. Subject Treatment (2-3 sentences)
"The [INPUT_IMAGE_SUBJECT] is positioned at [exact position %, e.g., 50% horizontal, 52% vertical]. [State, stability, motion characteristics]. The [INPUT_IMAGE_SUBJECT] retains its original texture and details but is integrated into this environment via matching lighting and color grading."
â†’ Specify exact numerical positions

### 5. Lighting & Color (2-3 sentences)
"[Light type] from [exact direction], [effect]. Key light adapted to match [INPUT_IMAGE] direction. Color palette: [specific hex colors from INPUT_IMAGE], [tonal range], delivering cinematic skin tones with characteristic ARRI color science."
â†’ MUST specify exact lighting direction and colors from [INPUT_IMAGE]

## Critical Rules
- ALWAYS include the ARRI camera/lens phrase
- Use [INPUT_IMAGE_SUBJECT] variable throughout (never replace with actual subject name)
- Specify positions as percentages (e.g., "positioned at 60% frame height")
- Include specific hex color codes when possible
- Avoid vague expressions:
  âŒ "beautiful" â†’ âœ… "sophisticated luxury aesthetic with warm amber tones"
  âŒ "nice lighting" â†’ âœ… "soft directional lighting from upper-left at 45Â° angle"
- 180-220 words (max 1800 characters) for prompt_en (IMPORTANT: Generate detailed, comprehensive prompts)
- Describe a still image moment
- No negative expressions
"""
    else:
        system_prompt = f"""
You are a professional advertising creative director specializing in luxury brand imagery.
Create high-quality prompts for generating scene images with ARRI cinematic look.

## MANDATORY: Camera & Lens Look
ALL generated prompts MUST include this camera specification:
{arri_look_phrase}

## Input
You will receive structured input with the following fields:
- Subject: Main subject to photograph
- Position: Subject's position in frame (e.g., "centered at 50% horizontal, 52% vertical")
- Background: Setting/environment
- Lighting: Light type and direction
- Color palette: Main colors (use hex codes when possible)
- Mood: Emotional atmosphere
- Additional notes: Extra instructions

## Output: 5-Stage Nanobanana Structure

### 1. Main Concept (1-2 sentences)
"This image employs [technique] to create [environment] for [subject]. Shot on ARRI ALEXA 35 with ARRI Signature Prime lens."
â†’ Include ARRI camera specification

### 2. Visual Signature (1 sentence)
"[Subject] at [exact position %], surrounded by [elements], [color scheme with hex codes]."
â†’ Use specific percentages for positions

### 3. Technical Approach (1-2 sentences)
"[Photography type] with [techniques]. ArriRaw to Rec709 color conversion applied, delivering cinematic skin tones and characteristic ARRI color science."
â†’ Include ARRI color science

### 4. Subject Treatment (2-3 sentences)
"The subject is positioned at [exact position %, e.g., 50% horizontal, 52% vertical]. [State: upright/tilted/floating], [stability: stable/dynamic], [motion: static/blur amount]. Maximum sharpness with crisp edges, detail visibility preserved."
â†’ Specify exact numerical positions

### 5. Lighting & Color (2-3 sentences)
"[Light type: softbox/key light/rim light] from [exact direction: upper-left at 45Â°], creating [effect: sculptural shadows/rim highlights]. Color palette: [hex codes], [tonal range: cool/warm], smooth roll-off in highlights and rich shadow detail."
â†’ Specify exact lighting angles and hex colors

## Critical Rules
- ALWAYS include the ARRI camera/lens phrase in the prompt
- Specify positions as percentages (e.g., "positioned at 60% frame height, 50% horizontal")
- Include specific hex color codes (e.g., "#3d5a6b steel blue")
- Specify lighting angles (e.g., "from upper-left at 45Â° angle")
- Avoid vague expressions:
  âŒ "beautiful" â†’ âœ… "sophisticated luxury aesthetic with warm amber #CC8844 tones"
  âŒ "nice lighting" â†’ âœ… "soft directional lighting from upper-left at 45Â° angle"
  âŒ "centered" â†’ âœ… "positioned at 50% horizontal, 48% vertical"
- 180-220 words (max 1800 characters) for prompt_en (IMPORTANT: Generate detailed, comprehensive prompts)
- Describe a still image moment
- No negative expressions
"""

    # å‚ç…§ç”»åƒã®æœ‰ç„¡ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆ
    if reference_image_url:
        user_prompt = f"""
[INPUT_IMAGE PROVIDED - This is [INPUT_IMAGE]. Analyze carefully.]

The subject in [INPUT_IMAGE] is [INPUT_IMAGE_SUBJECT].
Use structured input below to understand what [INPUT_IMAGE_SUBJECT] represents:

Structured Input:
{input_text}
Aspect ratio: {aspect_ratio} ({aspect_desc})

## Task
Generate image prompts that:
1. Use [INPUT_IMAGE_SUBJECT] variable to refer to the subject (do NOT replace with actual name)
2. Match the lighting, colors, and style from [INPUT_IMAGE]
3. Create an environment around [INPUT_IMAGE_SUBJECT] as specified in structured input
4. Include the ARRI camera/lens look phrase

Output in JSON format:
{{
  "prompt_ja": "å‚ç…§ç”»åƒã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ´»ã‹ã—ãŸ[INPUT_IMAGE_SUBJECT]ã®èª¬æ˜ï¼ˆ1-2æ–‡ï¼‰",
  "prompt_en": "Detailed 5-stage prompt using [INPUT_IMAGE_SUBJECT] variable, 180-220 words (max 1800 characters), MUST include ARRI camera phrase"
}}

IMPORTANT: In prompt_en, keep [INPUT_IMAGE_SUBJECT] as a literal variable, not replaced with actual subject name.
"""
    else:
        user_prompt = f"""
{input_text}
Aspect ratio: {aspect_ratio} ({aspect_desc})

Generate image prompts following the 5-stage nanobanana structure.
MUST include the ARRI camera/lens phrase in prompt_en.

Output in JSON format:
{{
  "prompt_ja": "æ—¥æœ¬èªã§ã®ç°¡æ½”ãªèª¬æ˜ï¼ˆ1-2æ–‡ï¼‰",
  "prompt_en": "Detailed 5-stage prompt, 180-220 words (max 1800 characters), MUST include: Shot on ARRI ALEXA 35 with ARRI Signature Prime lens. ArriRaw to Rec709 color conversion applied..."
}}
"""

    try:
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰ï¼ˆå‚ç…§ç”»åƒãŒã‚ã‚‹å ´åˆã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼‰
        contents = []

        if reference_image_url:
            # å‚ç…§ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            logger.info(f"Downloading reference image for prompt generation: {reference_image_url}")
            try:
                async with httpx.AsyncClient() as http_client:
                    img_response = await http_client.get(reference_image_url, timeout=30.0)
                    img_response.raise_for_status()
                    image_data = img_response.content

                # ç”»åƒã‚’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«è¿½åŠ 
                contents.append(types.Part.from_bytes(data=image_data, mime_type="image/jpeg"))
                logger.info("Reference image added to prompt generation request")
            except Exception as e:
                logger.warning(f"Failed to download reference image, proceeding without it: {e}")

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        contents.append(user_prompt)

        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=contents,
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                response_mime_type="application/json",
            )
        )

        result_text = response.text.strip()
        if result_text.startswith("```"):
            lines = result_text.split("\n")
            result_text = "\n".join(lines[1:-1])

        result = json.loads(result_text)

        if isinstance(result, list):
            result = result[0] if result else {}

        prompt_ja = result.get("prompt_ja", subject)
        prompt_en = result.get("prompt_en", "")

        if not prompt_en:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ§‹é€ åŒ–å…¥åŠ›ã‹ã‚‰ç›´æ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            prompt_en = _build_fallback_prompt_from_structured(structured_input, aspect_ratio)

        logger.info(f"Generated image prompt from structured input: {prompt_en[:100]}...")
        return prompt_ja, prompt_en

    except Exception as e:
        logger.exception(f"Failed to generate image prompt from structured input: {e}")
        prompt_ja = subject
        prompt_en = _build_fallback_prompt_from_structured(structured_input, aspect_ratio)
        return prompt_ja, prompt_en


def _build_fallback_prompt_from_structured(structured_input: dict, aspect_ratio: str) -> str:
    """æ§‹é€ åŒ–å…¥åŠ›ã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆARRI lookå«ã‚€ï¼‰"""
    subject = structured_input.get("subject", "subject")
    position = structured_input.get("subject_position", "positioned at 50% horizontal, 50% vertical")
    background = structured_input.get("background", "clean studio background")
    lighting = structured_input.get("lighting", "soft directional lighting from upper-left at 45Â° angle")
    color_palette = structured_input.get("color_palette", "neutral tones with subtle warm highlights")
    mood = structured_input.get("mood", "sophisticated professional aesthetic")

    return (
        f"This image employs precision studio photography to create a {mood} scene featuring {subject}. "
        f"Shot on ARRI ALEXA 35 with ARRI Signature Prime lens. "
        f"The subject is {position} with {background}. "
        f"ArriRaw to Rec709 color conversion applied, delivering cinematic skin tones, "
        f"natural color rendition, and characteristic ARRI color science with smooth roll-off in highlights and rich shadow detail. "
        f"The subject is sharp and well-defined with crisp edges, maximum detail visibility. "
        f"{lighting} creating sculptural shadows. "
        f"Color palette: {color_palette}. "
        f"High quality, 8K resolution, {aspect_ratio} aspect ratio."
    )


async def _generate_prompt_from_description(
    client,
    description_ja: str | None,
    dialogue: str | None,
    aspect_ratio: str,
    aspect_desc: str,
) -> tuple[str, str]:
    """
    å¾“æ¥ãƒ¢ãƒ¼ãƒ‰: è„šæœ¬ã¨ã‚»ãƒªãƒ•ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
    """
    # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®æ§‹ç¯‰ï¼ˆè„šæœ¬ãŒãƒ¡ã‚¤ãƒ³ã€ã‚»ãƒªãƒ•ã¯è£œåŠ©ï¼‰
    input_parts = []
    if description_ja:
        input_parts.append(f"è„šæœ¬: {description_ja}")
    if dialogue:
        input_parts.append(f"ã‚»ãƒªãƒ•: {dialogue}")

    input_text = "\n".join(input_parts) if input_parts else "ï¼ˆå…¥åŠ›ãªã—ï¼‰"

    # nanobanana 5æ®µéšæ§‹é€ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’çµ„ã¿è¾¼ã‚“ã ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆARRI lookå¿…é ˆï¼‰
    system_prompt = """
ã‚ãªãŸã¯ãƒ©ã‚°ã‚¸ãƒ¥ã‚¢ãƒªãƒ¼ãƒ–ãƒ©ãƒ³ãƒ‰å°‚é–€ã®åºƒå‘Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚
CMç”¨ã®ã‚·ãƒ¼ãƒ³ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®é«˜å“è³ªãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## å¿…é ˆ: Camera & Lens Look
ã™ã¹ã¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ä»¥ä¸‹ã®ã‚«ãƒ¡ãƒ©ä»•æ§˜ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„:
"Shot on ARRI ALEXA 35 with ARRI Signature Prime lens. ArriRaw to Rec709 color conversion applied, delivering cinematic skin tones, natural color rendition, and characteristic ARRI color science with smooth roll-off in highlights and rich shadow detail."

## å…¥åŠ›
- è„šæœ¬: ã‚·ãƒ¼ãƒ³ã®çŠ¶æ³èª¬æ˜ï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰
- ã‚»ãƒªãƒ•: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒè©±ã™å°è©ï¼ˆã‚ã‚Œã°å‚è€ƒã«ï¼‰

## å‡ºåŠ›
1. prompt_ja: ã‚·ãƒ¼ãƒ³ã®è¦–è¦šçš„ãªèª¬æ˜ï¼ˆæ—¥æœ¬èªã€1-2æ–‡ï¼‰
2. prompt_en: 5æ®µéšæ§‹é€ ã«åŸºã¥ã„ãŸè©³ç´°ãªè‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ180-220èªï¼ˆæœ€å¤§1800æ–‡å­—ï¼‰ã€ARRI lookå¿…é ˆï¼‰

## 5æ®µéšæ§‹é€ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆprompt_enç”¨ï¼‰

è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ä»¥ä¸‹ã®æ§‹é€ ã‚’1ã¤ã®æ®µè½ã¨ã—ã¦é€£çµã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

### 1. Main Conceptï¼ˆ1-2æ–‡ï¼‰
ã€ŒThis image employs [technique] to create [environment/effect]. Shot on ARRI ALEXA 35 with ARRI Signature Prime lens.ã€
â†’ ARRIã‚«ãƒ¡ãƒ©ä»•æ§˜ã‚’å«ã‚ã‚‹

### 2. Visual Signatureï¼ˆ1æ–‡ï¼‰
ã€Œ[Primary element] at [exact position %, e.g., 50% horizontal, 48% vertical], [surrounded by/featuring] [secondary elements] [color scheme with hex codes].ã€
â†’ ä½ç½®ã¯ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã§æŒ‡å®š

### 3. Technical Approachï¼ˆ1-2æ–‡ï¼‰
ã€Œ[Photography type] with [techniques]. ArriRaw to Rec709 color conversion applied, delivering cinematic skin tones and characteristic ARRI color science.ã€
â†’ ARRIã‚«ãƒ©ãƒ¼ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã‚’å«ã‚ã‚‹

### 4. Subject Treatmentï¼ˆ2-3æ–‡ï¼‰
- è¢«å†™ä½“ã®æ­£ç¢ºãªä½ç½®ï¼ˆä¾‹: positioned at 50% horizontal, 52% verticalï¼‰
- çŠ¶æ…‹ã€å®‰å®šæ€§ã€ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ç‰¹æ€§
- ã‚¨ãƒƒã‚¸ã®è³ªã€è©³ç´°ã®è¦–èªæ€§

### 5. Lighting & Colorï¼ˆ2-3æ–‡ï¼‰
- ç…§æ˜: ã€Œ[Light type: softbox/key light] from [exact direction: upper-left at 45Â°], creating [effect]ã€
- ã‚«ãƒ©ãƒ¼: ã€ŒColor palette: [hex codes], [tonal range], smooth roll-off in highlights and rich shadow detailã€
â†’ æ­£ç¢ºãªç…§æ˜è§’åº¦ã¨HEXã‚«ãƒ©ãƒ¼ã‚’æŒ‡å®š

## é‡è¦ãªãƒ«ãƒ¼ãƒ«
- å¿…ãšARRIã‚«ãƒ¡ãƒ©/ãƒ¬ãƒ³ã‚ºãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å«ã‚ã‚‹
- æ›–æ˜§ãªè¡¨ç¾ã‚’é¿ã‘ã‚‹:
  âŒ "beautiful" â†’ âœ… "sophisticated luxury aesthetic with warm amber #CC8844 tones"
  âŒ "nice lighting" â†’ âœ… "soft directional lighting from upper-left at 45Â° angle"
  âŒ "centered" â†’ âœ… "positioned at 50% horizontal, 48% vertical"
- é…ç½®ã¯å¿…ãšãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã§æŒ‡å®š
- ç…§æ˜ã¯è§’åº¦ã‚’æ˜ç¤ºï¼ˆä¾‹: from upper-left at 45Â° angleï¼‰
- è‰²ã¯HEXã‚³ãƒ¼ãƒ‰ã‚’å«ã‚ã‚‹ï¼ˆä¾‹: #3d5a6b steel blueï¼‰
- 180-220èªï¼ˆæœ€å¤§1800æ–‡å­—ï¼‰
- é™æ­¢ç”»ã¨ã—ã¦æˆç«‹ã™ã‚‹ç¬é–“ã‚’æå†™
"""

    user_prompt = f"""
{input_text}
ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”: {aspect_ratio}ï¼ˆ{aspect_desc}ï¼‰

ä¸Šè¨˜ã‹ã‚‰ç”»åƒç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
prompt_enã«ã¯å¿…ãšARRIã‚«ãƒ¡ãƒ©/ãƒ¬ãƒ³ã‚ºãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å«ã‚ã¦ãã ã•ã„ã€‚

JSONå½¢å¼ã§å‡ºåŠ›:
{{
  "prompt_ja": "ã‚·ãƒ¼ãƒ³ã®è¦–è¦šçš„èª¬æ˜ï¼ˆ1-2æ–‡ï¼‰",
  "prompt_en": "5æ®µéšæ§‹é€ ã®è©³ç´°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ180-220èªï¼ˆæœ€å¤§1800æ–‡å­—ï¼‰ï¼‰ã€‚å¿…ãšå«ã‚ã‚‹: Shot on ARRI ALEXA 35 with ARRI Signature Prime lens..."
}}
"""

    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=user_prompt,
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                response_mime_type="application/json",
            )
        )

        result_text = response.text.strip()
        # ```json ... ``` ã‚’é™¤å»
        if result_text.startswith("```"):
            lines = result_text.split("\n")
            result_text = "\n".join(lines[1:-1])

        result = json.loads(result_text)

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãƒªã‚¹ãƒˆã®å ´åˆã¯æœ€åˆã®è¦ç´ ã‚’ä½¿ç”¨
        if isinstance(result, list):
            result = result[0] if result else {}

        prompt_ja = result.get("prompt_ja", description_ja or dialogue or "ã‚·ãƒ¼ãƒ³ç”»åƒ")
        prompt_en = result.get("prompt_en", "")

        if not prompt_en:
            # è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç©ºã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆARRI lookå«ã‚€ï¼‰
            prompt_en = (
                f"This image employs cinematic photography to create a sophisticated scene depicting {prompt_ja}. "
                f"Shot on ARRI ALEXA 35 with ARRI Signature Prime lens. "
                f"The subject is positioned at 50% horizontal, 48% vertical with balanced composition. "
                f"ArriRaw to Rec709 color conversion applied, delivering cinematic skin tones, "
                f"natural color rendition, and characteristic ARRI color science with smooth roll-off in highlights and rich shadow detail. "
                f"Soft directional lighting from upper-left at 45Â° angle creating sculptural shadows. "
                f"Warm neutral color palette with subtle highlights, high quality, 8K resolution."
            )

        logger.info(f"Generated image prompt: {prompt_en[:100]}...")
        return prompt_ja, prompt_en

    except Exception as e:
        logger.exception(f"Failed to generate image prompt: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ARRI lookå«ã‚€
        fallback_ja = description_ja or dialogue or "ã‚·ãƒ¼ãƒ³ç”»åƒ"
        fallback_en = (
            f"This image employs cinematic photography to create a sophisticated scene depicting {fallback_ja}. "
            f"Shot on ARRI ALEXA 35 with ARRI Signature Prime lens. "
            f"The subject is positioned at 50% horizontal, 48% vertical with balanced composition. "
            f"ArriRaw to Rec709 color conversion applied, delivering cinematic skin tones, "
            f"natural color rendition, and characteristic ARRI color science with smooth roll-off in highlights and rich shadow detail. "
            f"Soft directional lighting from upper-left at 45Â° angle creating sculptural shadows. "
            f"Warm neutral color palette with subtle highlights, high quality, 8K resolution."
        )
        return fallback_ja, fallback_en


async def convert_to_flux_json_prompt(
    description_ja: str,
    negative_prompt_ja: str | None = None,
    aspect_ratio: str = "9:16"
) -> dict:
    """
    æ—¥æœ¬èªã®èª¬æ˜æ–‡ã‚’FLUX.2ç”¨ã®JSONæ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè‹±èªï¼‰ã«å¤‰æ›

    Args:
        description_ja: æ—¥æœ¬èªã®ç”»åƒèª¬æ˜
        negative_prompt_ja: æ—¥æœ¬èªã®ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        aspect_ratio: ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯” ("9:16" or "16:9")

    Returns:
        dict: {
            "json_prompt": str,  # JSONå½¢å¼ã®è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            "negative_prompt_en": str | None,  # è‹±èªã®ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            "preview": dict  # ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ï¼‰
        }
    """
    client = get_gemini_client()

    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã«å¿œã˜ãŸæ§‹å›³ãƒ’ãƒ³ãƒˆ
    composition_hint = "vertical portrait composition, 9:16 aspect ratio" if aspect_ratio == "9:16" else "horizontal landscape composition, 16:9 aspect ratio"

    system_prompt = f"""You are an expert prompt engineer for FLUX.2 image generation.
Convert the Japanese description into a structured JSON prompt in English.

IMPORTANT RULES:
1. Output ONLY valid JSON, no markdown code blocks or explanations
2. All values must be in English
3. Be specific and detailed in descriptions
4. Include cinematic/photography terms for professional quality
5. The composition should be: {composition_hint}

JSON Structure (use exactly these keys):
{{
  "scene": "Environment/setting description",
  "subject": "Main subject with detailed appearance",
  "style": "Visual style (e.g., cinematic, editorial, fine art)",
  "camera": "Camera settings (lens, angle, depth of field)",
  "lighting": "Lighting setup and quality",
  "color_palette": "Color scheme description or hex codes",
  "mood": "Emotional atmosphere",
  "quality": "Technical quality descriptors"
}}

Example output:
{{
  "scene": "Modern Tokyo cafe interior, large windows with morning sunlight",
  "subject": "Young Japanese woman in her 20s, wearing a cream knit sweater, holding a ceramic coffee cup, gentle smile, looking slightly off-camera",
  "style": "Editorial photography, natural and authentic, lifestyle aesthetic",
  "camera": "85mm f/1.4 lens, eye-level angle, shallow depth of field, subject in sharp focus",
  "lighting": "Soft natural window light from the left, subtle rim light, no harsh shadows",
  "color_palette": "Warm neutrals, cream #F5E6D3, soft brown #8B7355, white #FFFFFF",
  "mood": "Calm, contemplative, inviting warmth",
  "quality": "8K resolution, professional photography, high detail, clean composition"
}}"""

    try:
        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¤‰æ›
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=f"{system_prompt}\n\nJapanese description to convert:\n{description_ja}",
            config=types.GenerateContentConfig(temperature=0.4)
        )

        json_text = response.text.strip()
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
        if json_text.startswith("```"):
            json_text = json_text.split("```")[1]
            if json_text.startswith("json"):
                json_text = json_text[4:]
            json_text = json_text.strip()

        # JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ï¼ˆæ¤œè¨¼ï¼‰
        try:
            preview = json.loads(json_text)
        except json.JSONDecodeError:
            logger.warning(f"JSON parse failed, attempting to fix: {json_text[:100]}")
            # ä¿®å¾©ã‚’è©¦ã¿ã‚‹
            json_text = json_text.replace("'", '"')
            preview = json.loads(json_text)

        logger.info(f"Converted to FLUX JSON prompt: {json_text[:100]}...")

        # ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¤‰æ›ï¼ˆæŒ‡å®šãŒã‚ã‚‹å ´åˆï¼‰
        negative_prompt_en = None
        if negative_prompt_ja and negative_prompt_ja.strip():
            neg_response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=(
                    "Translate the following Japanese negative prompt to English. "
                    "Return ONLY a comma-separated list of terms to avoid in image generation. "
                    "Keep it concise and technical.\n\n"
                    f"Japanese: {negative_prompt_ja}"
                ),
                config=types.GenerateContentConfig(temperature=0.2)
            )
            negative_prompt_en = neg_response.text.strip()
            negative_prompt_en = negative_prompt_en.strip('"\'')
            logger.info(f"Converted negative prompt: {negative_prompt_en[:50]}...")

        return {
            "json_prompt": json_text,
            "negative_prompt_en": negative_prompt_en,
            "preview": preview
        }

    except Exception as e:
        logger.exception(f"Failed to convert to FLUX JSON prompt: {e}")
        raise ValueError(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
